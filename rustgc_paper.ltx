%&rustgc_paper_preamble
\endofdump

\begin{document}

\begin{abstract}
\noindent Rust is a non-Garbage Collected (GCed) language, but the lack of GC
makes expressing data-structures whose values have multiple owners awkward or inefficient.
In this paper we explore a new design for GC in Rust, identifying
the major challenge as finalisers: mapping existing Rust destructors
to GC finalisers seems natural but introduces subtle soundness, significant
performance, and irritating ergonomic issues. In this paper we introduce \ourgc, a new GC for Rust,
which provides solutions for each of these issues.
\end{abstract}

\maketitle


\section{Introduction}

Amongst the ways one can classify programming languages are whether they
are Garbage Collected (GCed) or not: GCed languages enable implicit memory management;
non-GCed languages require explicit memory management (e.g~\lstinline{C}'s \lstinline{malloc} /
\lstinline{free} functions). Rust's use of affine types and ownership does not
fit within this classification: it is not GCed but it has implicit memory management.
Most portions of most Rust programs are as
succinct as a GCed equivalent but ownership is too inflexible to express
data-structures that require multiple owners (e.g.~linked lists).
Workarounds have many downsides, from memory leaks, inflexibility, or complexity.

There have thus been a number of attempts to add GC to Rust. Typically these
introduce a user-visible type \lstinline{Gc<T>} which moves values of
type \lstinline{T} to the GC heap and then wraps a pointer to it. \lstinline{Gc<T>}
can be \emph{cloned} (i.e.~duplicated) at will by the user. When no
\lstinline{Gc<T>} values can be found, indirectly or directly, from the program's \emph{roots} (e.g.~global
variables, variables on the stack, etc.) to the underlying value then the
GC reclaims the relevant memory. However, existing GCs for Rust require sacrificing at
least one of soundness (e.g.~the Bronze GC~\cite{coblenz21bronze} cannot
track indirect references, so seemingly safe programs can fail due to
use-after-frees etc.), performance (\laurie{use a sensible figure from the
perf section}), or ergonomics (e.g.~requiring drop methods to be duplicated
as finalisers).

\jake{Right now the performance story is disappointing. We are 40\%
slower than regular \lstinline{Rc}, this is down how multi-threading user
threads are handled in BDWGC. The long and short of it is that we must disable
parallel marking in order to prevent severe lock contention on the heap when we
have more than one user thread. If we don't do this, multi-threaded
applications would slow down by around 10x!. However, by disabling parallel
marking we end up making regular single-threaded applications 20\% slower than they were
before. In other words, we've gone from a 20\% slowdown to a 40\% slowdown, and
I don't have a satisfying answer to this with BDWGC as our current collector
backend. (See \ref{graph:som_bar} / \ref{graph:som_bar_new} for a before and after comparison)}

In this paper we introduce \ourgc, a new GC for Rust. \ourgc's basic design is
similar to many other GCs for Rust but unlike previous GCs,
we identify the chief challenge for a sound,
performant, ergonomic GC for Rust as \emph{finalisers} (i.e.~code which is run just before a
value is collected by the GC). Rust uses \emph{destructors} (i.e.~code which
is run just before a value is freed by Rust's implicit memory management)
pervasively in the form of \lstinline{drop} methods. When a type
\lstinline{T} with a destructor is used with \lstinline{Gc<T>} it is natural
to map its destructor to a finaliser. However, this introduces three
fundamental problems:
(1) some safe destructors are not safe finalisers;
(2) finalisers can be run too early;
and (3) finalisers are prohibitively slower than destructors.

We introduce solutions for each of these problems:
(1) \emph{Finaliser Safety Analysis} (FSA)
extends Rust's type system to reject programs which use type \lstinline{T} with
\lstinline{Gc<T>} when \lstinline{T}'s destructor cannot be statically proven
to be safe to use as a finaliser;
(2) \emph{early finaliser prevention} inserts barriers that prevent factors
such as optimisations or register allocation from `tricking'
the GC into collecting values before they are dead;
and (3) \emph{finaliser elision} statically optimises away many finalisers,
by recognising that many destructors only free heap memory, and do not need
to be run in a GC context.

As well as solving these fundamental problems, we also introduce several
ergonomic improvements. The most visible is that we allow GC types to be
duplicated without requiring explicit calls to \texttt{clone}. Less visible is that we
extend FSA to enlarge the set of types whose destructors can be statically
proven to be safe finalisers.

\ourgc is implemented as a fork / extension of the standard Rust compiler
\rustc, using conservative GC in order to support a fully idiomatic Rust API
for \lstinline{Gc<T>}. \jake{I think a take-away from this work is that you can't have a sound, ergononic, and performant GC library without modifying the language itself}  Although conservative GC is used in widespread
programs such as Chrome and Safari, it is unsound in most languages
and compilers: thus \ourgc is sound only to the extent that
conservative GCs in general are sound (see~\cref{conservative_gc}). Since
conservative GC is widely used, we consider this a pragmatic trade-off:
we believe that one could create a variant of \ourgc with a
less idiomatic Rust API that would not require conservative GC if so desired.

Overall, while we do not pretend that \ourgc is a production-ready GC for Rust, it is
sufficiently polished to be usable for real programs: it has high-quality error
messages and reasonable performance. Although \ourgc is necessarily tied to
Rust, we believe that most of the techniques in this paper would generalise to
other ownership-based languages.

\begin{figure}
\begin{lstlisting}[
  language=Rust,
  caption={An example of GC in Rust. \laurie{we need a good example here,
    ideally one that we can build upon later in the paper}},
  label={fig:first_example}]
struct Node {data: i32, next: Option<Gc<RefCell<Node>>>}

fn main() {
    let b = Gc::new(RefCell::new(Node {data: 1, next: None}));
    let a = Gc::new(RefCell::new(Node {data: 2, next: Some(b)}));
    b.borrow_mut().next = Some(a);
}
\end{lstlisting}
\end{figure}


\section{Background}

In this section we briefly show why some data structures that are easy to
write in GCed languages are challenging in Rust.

Rust uses affine types~\citep{pierce04advanced} and \emph{ownership}
to statically guarantee that: a \emph{value} has a
single owner (e.g.~a variable): an owner can \emph{move} that value to another owner; once
the owner goes out of scope, then the value's \lstinline{drop} method (its destructor)
is run and its memory can be reused\jake{phrasing doesn't quite work}. An owner can give references to a value
to other code, subject to the following static restrictions: there can be
multiple read-only \emph{references} (`\lstinline{&}') to a value, or a single
read-write reference (`\lstinline{&mut}'); and references cannot outlast the owner. \jake{This is where we must make a decision about whether we use read-only / read-write or immutable / mutable terminology for the rest of the paper. I like the former, but I think the latter is more familiar to Rust programmers}

These basic rules allow many Rust programs to be written in a style that makes
them indistinguishable from their equivalents in GCed languages\jake{I'm not sure I agree, what does this mean?}. However,
data structures that require values to have more than one owner cannot
be directly expressed in \emph{safe} Rust (i.e.~the subset of Rust that is fully
statically checked by the compiler). Instead, one must resort to indirect
and/or \emph{unsafe} (i.e.~not fully statically checked by the compiler) workarounds.

Perhaps the most widely used workaround is to use the reference counting type
\lstinline{Rc<T>} in Rust's standard library. For many non-cyclic
data-structures reference counting is a reasonable solution, but using it for
cyclic data-structures requires juggling strong and weak counts, which makes
it easy for values to live for shorter or longer than required. Another solution are
arenas which, having accumulated multiple values, free all of them at once: values
can no longer be reclaimed too early, but nor can they be freed early if they are
no longer needed. A type-based approach are \lstinline{GhostCell}s~\cite{yanovski21ghostcell},
which allow cyclic data-structures to be built by statically guaranteeing that
there is a single owner for the entire data-structure at any point in the
program. However, this prevents use cases where multiple owners (possibly,
though not necessarily, in different threads) need to read or mutate different
parts of a data-structure.

\laurie{jake: can we find a minimal example here? linked lists are an obvious
one, but they're too complicated i think: we can at least reference
\lstinline{https://rust-unofficial.github.io/too-many-lists/} as proof of that.
maybe the bronze GC paper has some good examples. otherwise, maybe we just
introduce a type `S' which can reference other `S`s and show the problems with
rc and rc/refcell?}\jake{How about the one in \autoref{fig:first_example}?}


\subsection{GC terminology}

\laurie{tracing vs.non-tracing}
\laurie{mutator vs.~non-mutator}

\jake{My preferences on terminology are as follows:}
\jake{Tracing vs Reference Counting.}
\jake{Mutator vs.~collector (when talking about threads)}
\jake{The term 'concurrent' should only be used when referring to, whether the
collector does stuff at the same time as the mutator threads. Alloy is
therefore never concurrent.}
\jake{Use parallel to refer to multiple mutator threads or multiple collector threads}.
\jake{Gc'd language vs non-GC'd language (instead of managed / unmanaged)}.


\section{Destructors vs.~Finalisers}
\label{sec:finaliser_issues}

In this section we look at the challenge of destructors for a GC for Rust.
Rust uses destructors pervasively in the form of \emph{drop methods}.
When the owner of a value goes out of scope, or if the value is explicitly
passed to the \lstinline{std::mem::drop} function, then the value's drop
method is called.\jake{This is just an empty method which makes the value go out of scope, so really it's the same as the first point. There's nothing special about this function. The interesting one is \lstinline{drop_in_place} that takes a non-owned pointer, but this will complicate things in this paragraph. I suggest removing the bit about \lstinline{std::mem::drop} entirely.} For example, suitably simplified (e.g.~ignoring weak
counts), \lstinline{Rc<T>} has the following drop method:

\begin{lstlisting}[language=Rust,numbers=left]
impl<T> Drop for Rc<T> {
  fn drop(&must self) {
    self.inner.count -= 1;
    if self.inner.count == 0 {
      unsafe { Box::from_raw(self.inner.block);
    }
  }
}
\end{lstlisting}

In this example, \lstinline{Rc<T>} implements Rust's standard \lstinline{Drop}
\emph{trait} (roughly equivalent to a Java interface) which then requires
implementing a \lstinline{drop} method. \lstinline{Rc<T>} itself contains a reference count and a pointer to
a block of memory on the heap allocated with Rust's standard \lstinline{Box<T>}
type (which allocates heap memory). \lstinline{Rc<T>}'s drop method
first decrements the reference count. If that count
goes to zero then the underlying \lstinline{Box<T>} is reconstituted from its raw pointer
(with \lstinline{from_raw}), and immediately goes out of scope, implicitly
calling \lstinline{Box<T>}'s drop method. This will then \jake{recursively?} call \lstinline{T}'s
drop method and then deallocate the underlying heap memory.

Rust has simple, well-defined rules that allow programmers to accurately reason
about when destructors are called\footnote{There are some exceptions: for
example Rust's `temporary lifetime extension' can delay destruction, but how
long temporary values are kept alive for is currently unspecified.}. This then
enables a style of programming that originated in C++ called RAII (Resource
Acquisition Is Initialization)~\cite[Section~14.4]{stroustrup97c++}.
For example, Rust's standard \lstinline{Mutex} type has a method
\lstinline{lock} which returns a \lstinline{MutexGuard} value;
\lstinline{MutexGuard}'s drop method automatically unlocks the
\lstinline{Mutex}; and so a programmer can rely on a locked mutex being unlocked at
a predictable point, without having to explicitly call an unlock
method.

At first destructors can appear identical to GC finalisers: both concepts run user code
just before the memory a value occupies can be reused by the wider system.
However, there is one obvious difference between the two: while Rust defines precisely
when a destructor will be run, GC finalisers are
only run at some unspecified point in time after the last
reference to a GCed value is dropped.

If we assume the existence in Rust of a `fully' GCed type \lstinline{Gc<T>}
this immediately raises an important question: when are \lstinline{Gc<T>}
finalisers run? When the reference count in an \lstinline{Rc<T>} goes to 0, \lstinline{T}'s
drop method is called immediately. But when the last reference to an \lstinline{Gc<T>}
is lost, \lstinline{T}'s drop method may not be called for some time. Indeed,
`when` is not the only question this raises: where are finalisers run? If we
were to run them on the same thread as mutator code is running,
then the finaliser can cause a deadlock if it tries to access a resource that
the normal code holds (see~\cref{finaliser_thread} for more details). However,
not all \lstinline{drop} methods can safely be run on another thread.

In addition to `when' and `where', finalisers have other subtle
issues~\cite{boehm03destructors}. For example, what should we do with finalisers
in values that form a cycle? In such a situation, a finaliser can reference
values that are partly, or wholly, `finalised'. Other issues include the
ability of finalisers to `resurrect' values by
copying their GC reference to the main part of a program.

Over time
finalisers have come to be viewed with increasing suspicion. Java,
for example, has deprecated, and intends eventually removing, per-type
finalisers, in favour of mechanisms such as per-object `cleaners', whose API
prevents problems such as object resurrection~\cite{goetz21deprecated}.
\laurie{we need to briefly talk about how a couple of other rust gcs handle (or
presumably don't) finalisation}


\section{\ourgc: Basic Design Choices and Implementation Approach}

The fundamental aim of \ourgc is to add GC for Rust in a way that is
sound, performant, and ergonomic. In this section we outline \ourgc's basic design choices
and implementation approach -- the rest of the paper then goes into detail on the more
advanced aspects.


\subsection{Basic Design Choices}

\ourgc provides a \lstinline{Gc<T>} type that exposes an API modelled on the
reference counting type \lstinline{Rc<T>} from Rust's standard library, because
\lstinline{Rc<T>}: is conceptually
similar to \lstinline{Gc<T>}; is widely used in Rust code, and so its API is
familiar; and its API reflects long-term experience about what Rust programmers
need from such a type.

When a user calls \lstinline{Gc::new(v)}, the value \lstinline{v} is
moved to the GC heap: the \lstinline{Gc<T>} value returned to the user is a
simple wrapper around a pointer to \lstinline{v} on the GC heap. Dereferencing
a \lstinline{Gc<T>} produces an immutable Rust reference to the underlying
value: there is no way to directly mutate GCed values and users must use other
standard Rust types (e.g.~\lstinline{Mutex<T>}) that enable \emph{interior mutability}.

One feature that \ourgc explicitly adopts is \lstinline{Rc<T>}'s
ability to be transformed into a raw pointer (\lstinline{into_raw}) and
back (\lstinline{from_raw}). These functions have a number of uses, ranging
from pointer tagging (frequently used in GCing language VMs \laurie{find a citation}\jake{Are you looking for a paper? Something like this? \lstinline{https://v8.dev/blog/pointer-compression}?})
to passing values to foreign code. However, these features have
a profound impact because Rust allows
raw pointers to be converted to the integer type \lstinline{usize} and
back. In other words, if \ourgc wants to allow GCed values to be converted
into pointers and back again, it must be able to track
pointers to GC values that are stored in Rust \lstinline{usize} values.

\label{conservative_gc}
Having acknowledged that pointers may end up disguised as integers, it is then
inevitable that \ourgc must be a \emph{conservative} GC. In essence, such GCs,
starting from the roots, treat machine words as possible pointers: if
a word does point into a GCed block of memory, then that block itself is also
conservatively scanned. Since a conservative GC doesn't know if a word is a
pointer from the program's perspective, or just happens to be a sequence of
bits that also happens to make it a valid pointer, this over-approximates the
\emph{live set} (i.e.~the blocks that the GC will not reclaim). However, the
most extensive study we know of suggests the false detection rate in Java
programs is under 0.01\% of live objects~\cite{shahriyar14fast}.

Conservative GC occupies a grey zone in programming language semantics. In
most languages (including the semantics of most compiler IRs) conservative GC
relies on undefined behaviour. \ourgc's use of conservative GC means \ourgc
is, formally speaking, unsound. However, conservative GC is widely used,
including in the two most widespread web browsers: Chrome uses it in its rendering
engine~\citep{ager13oilpan} and WebKit uses it in its JavaScript
VM~\citep{pizlo17riptide}. Even in 2024, we lack good
alternatives: there is no cross-platform API for precise GC; and while some
compilers such as LLVM provide some support for GC features, we have found them
incomplete and buggy. Practically speaking, despite the potential soundness
worries, conservative GC remains a widely used technique.

Conservative GC enables \ourgc to make a useful ergonomic improvement over
most other GCs for Rust. \lstinline{Rc<T>} is a \emph{cloneable} type: it can be duplicated, but doing
so requires executing arbitrary user code (in this case to increment / decrement
reference counts). To make the possible run-time cost of this clear, Rust has
no direct syntax for cloning: users must explicitly call \lstinline{Rc::clone(&x)}
to duplicate a value \lstinline{x}. In contrast, since \lstinline{Gc<T>} is just a wrapper around a pointer it
is not just cloneable but also \emph{copyable}: duplication only requires copying
bytes (i.e.~no arbitrary user code need be executed). Copying is implied by assignment,
reducing the need for a function call entirely\footnote{The lengthier
syntax \lstinline{y = Gc::clone(&x)} is available, since every copyable type is
also cloneable.}. Across large code bases, this seemingly minor syntactic
convenience can be a real boon because copying frees the programmer from Rust's normal
restrictions on ownership and moving. It also reflects an underlying
semantic difference: duplicating a \lstinline{Gc<T>} is a cheaper and simpler operation
than duplicating an \lstinline{Rc<T>}.


\subsection{Implementation Approach}

\laurie{rustc fork, BDWGC, GcBox, etc.}

Currently, \ourgc uses the well-known
Boehm-Demers-Weiser GC (\boehm)~\citep{boehm88garbage}, although the major
challenges we tackle in this paper will remain the same no matter what
underlying conservative GC is used.

\ourgc introduces a new smart pointer type, \lstinline{Gc<T>}, which provides
shared ownership of a value of type \lstinline{T} allocated in the heap and
managed by a garbage collector. Consider a simple example and its corresponding
representation in memory:

\begin{minipage}[c]{0.5\linewidth}
\begin{lstrustsmall}
use std::gc::Gc;

fn main() {
    let a = Gc::new(123);
}
\end{lstrustsmall}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
    \includegraphics[width=1\textwidth]{images/alloy_basic_gc_1}
\end{minipage}

This creates a garbage collected object which contains the \lstinline{u64} value
\lstinline{123}. A \lstinline{Gc}'s data is stored in a \lstinline{GcBox}
internally. \lstinline{GcBox}es are managed by the collector, though this is not visible to the
user.

\lstinline{Gc} references are copyable (i.e.~they implement the \lstinline{Copy}
trait), with copied references pointing to the same object in the heap:

\begin{minipage}[c]{0.5\linewidth}
\begin{lstrustsmall}
fn main() {
    let a = Gc::new(123);
    let b = a;
}
\end{lstrustsmall}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
    \includegraphics[width=1\textwidth]{images/alloy_basic_gc_2}
\end{minipage}

This makes \lstinline{Gc} more ergonomic to use than \lstinline{Rc}, as there is
no need to call \lstinline{clone} on a \lstinline{Gc} to obtain another
reference to its data.

The \lstinline{GcBox} referenced by a \lstinline{Gc} is guaranteed not to be
freed while there are still references to it. When there are no longer any
references, the collector will reclaim it at some point in the future. The
garbage collector runs intermittently in the background, so \lstinline{Gc}
objects may live longer than they need to.

\subsubsection{Dereferencing}

A \lstinline{Gc<T>} dereferences to \lstinline{T} with the dereference
(\lstinline{*}) operator:

\begin{minipage}[c]{0.5\linewidth}
\begin{lstrustsmall}
fn main() {
    let a = Gc::new(123);
    let b = *a;
    foo(b);
}

fn print(int: u64) {
    println!("{}", int);
}
\end{lstrustsmall}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
    \includegraphics[width=1\textwidth]{images/alloy_basic_gc_3}
\end{minipage}

Here, the value can be copied out of the \lstinline{Gc} into \lstinline{b}
because \lstinline{u64}s are copyable. The \lstinline{Gc} type also allows the
dot operator to be used for calling methods of type \lstinline{T} on a
\lstinline{Gc<T>}:

\begin{lstrustsmall}
struct Wrapper(u64);

impl Wrapper {
    fn foo(&self) {
        ...
    }
}

fn main() {
    let a = Gc::new(Wrapper(123));
    a.foo();
}
\end{lstrustsmall}

\subsection{Mutation}

There is no way to mutate, or obtain a mutable reference (\lstinline{&mut T})
to a \lstinline{Gc<T>} once it has been allocated. This is because mutable
references must not alias with any other references, and there is no way to know
at compile-time whether there is only one \lstinline{Gc} reference to the data.

As with other shared ownership types in Rust, interior mutability
(\autoref{intmut}) must be used when mutating the contents inside a
\lstinline{Gc}:

\begin{lstrustsmall}
fn main() {
    let a = Gc::new(RefCell::new(123));
    *a.borrow_mut() = 456; // Mutate the value inside the GC
}
\end{lstrustsmall}

\subsubsection{Concurrency}
\label{sendsync}

The Rust type system is able to guarantee statically that values are being used
in a thread-safe way. It does this with the use of two special "marker" traits:
\lstinline{Send} and \lstinline{Sync}.

A value whose type implements the \lstinline{Send} trait can be transferred to
other threads. Almost all types in Rust implement the \lstinline{Send} trait
save for a few exceptions. One such exception is the \lstinline{Rc<T>}
(reference counting) type. This does \emph{not} implement \lstinline{Send}
because it does not perform count operations on its underlying contents
atomically. If it were sent to another thread, it could race if both threads
tried to update the count simultaneously. In contrast, the \lstinline{Arc<T>}
type (an atomic implementation of \lstinline{Rc<T>}) does implement
\lstinline{Send} provided its inner type \lstinline{T} is also \lstinline{Send}.

The corollary to \lstinline{Send} is the \lstinline{Sync} trait, which can be
implemented on types whose values perform mutation in a thread-safe manner. For
example, a \lstinline{Mutex<T>} implements \lstinline{Sync} because it provides
exclusive access to its underlying data atomically. On the other hand, a
\lstinline{RefCell} (discussed in \autoref{intmut}) does not implement
\lstinline{Sync} because it only provides single-threaded interior
mutability. This is because its underlying mechanism to increment and decrement
borrow counts are not performed atomically.

The \lstinline{Send} and \lstinline{Sync} marker traits are a special kind of
trait known as \emph{auto traits}. This means that they are automatically
implemented for every type, unless the type, or a type it contains, has
explicitly opted out. Types can be opted out via a \emph{negative impl}:

\begin{lstrustsmall}
    impl !Send for T {}
\end{lstrustsmall}

If, for example, a struct \lstinline{S} contains a field of type \lstinline{T}
from the example above, then the entire struct \lstinline{S} will also not
implement \lstinline{Send}.

\lstinline{Send} and \lstinline{Sync} can be manually implemented on types, but
doing so requires \lstinline{unsafe} Rust code since the user must guarantee it
is safe to use in a multi-threaded context.

\ourgc's \lstinline{Gc} reference is fundamentally thread-safe: if \lstinline{T}
implements \lstinline{Send} + \lstinline{Sync}, then \lstinline{Gc<T>} will too.
The \lstinline{Gc<T>} type therefore conditionally implements \lstinline{Send} +
\lstinline{Sync} depending on \lstinline{T}:

\begin{lstrustsmall}
unsafe impl<T: Send> Send for Gc<T> {}
unsafe impl<T: Sync> Sync for Gc<T> {}
\end{lstrustsmall}

\subsubsection{Conservative GC}

\label{ourgc:soundness}

\ourgc is a conservative GC (\autoref{bg:css}), which means that by nature, it
is unsound. This is because, technically speaking, the way conservative GC works
violates the rules of most languages, most compilers, and most operating
systems. In very rare cases, compilers have been known to perform
optimisations which can obfuscate pointers from the
collector~\citep{chromium20cssbug}. Fortunately, the ubiquity of conservative GCs in industrial strength
VMs means that in practise it is well supported. If one can accept this caveat,
\ourgc is otherwise correct-by-design provided that users do not hide,
accidentally or otherwise, pointers from the GC. Programming techniques which
rely on \emph{pointer obfuscation}~\citep{boehm96simple} are therefore not
allowed in \ourgc. This rules out the use of certain data structures such as XOR
lists.

When \ourgc performs a collection, it must first identify the roots from which
the rest of the object graph is traced. Such roots exist on the call stack, in
registers, and in segments of the program which store global values. When a
collection is scheduled, the BDWGC spills register values to the stack so that
their contents can be scanned for pointers along with the rest of the
stack~\citep{boehm88garbage}. The call stack is exhaustively examined for
possible pointers to instances of objects, with each aligned word on the stack
is checked to see whether it points to an instance of an object: if it does,
that object is considered a root. \autoref{fig:roots} shows an example of what
\ourgc considers roots to garbage collected objects.

\begin{figure}
% Hack to get the listing in the figure
\newsavebox{\rootlisting}
\begin{lrbox}{\rootlisting}
\begin{lstrustsmallnonums}
// `a` exists on the stack.
let a = Gc::new(1);
let b = a; // obtain copy



let c = Gc::new(Gc::new(2));
// obtain a rust (&) ref
let d = c.as_ref();
\end{lstrustsmallnonums}
\end{lrbox}
    \centering
    \subfloat[\centering Roots on the stack]{\usebox{\rootlisting}}
    \hfill
    \subfloat[\centering Representation in memory]{
        \includegraphics[width=0.5\textwidth]{images/gc_roots.pdf}
    }
    \caption{An example showing values on the stack which are considered
    roots to \lstinline{GcBox}es.}
    \label{fig:roots}
\end{figure}

\begin{figure}[t]
\begin{lstrustsmall}
use std::gc::GcAllocator;

#[global_allocator]
static ALLOCATOR: GcAllocator = GcAllocator;

fn main() {
    ...
}
\end{lstrustsmall}
    \caption{Setting the global allocator to use the BDWGC in \ourgc using
    Rust's \lstinline{global_allocator} attribute.}
\label{fig:allocator}
\end{figure}

\subsubsection{Garbage collected objects in other heap objects}

In \ourgc, references to garbage collected objects can be stored in
traditional, non-garbage-collected Rust objects:

\begin{lstrustsmall}
fn main() {
    let v = Vec::new();
    v.push(Gc::new(1));
    v.push(Gc::new(2));
}
\end{lstrustsmall}

Here, the vector contains two references to garbage collected objects which are
managed by \ourgc. Even though the vector itself is not garbage collected, its
backing store must still be traced during a collection in order to locate GC
objects. To support this, every allocation in a \ourgc must use the BDW
allocator -- even those which are not garbage collectable. This is because the
BDW allocator stores bookkeeping information such as mark bits and the memory
block size which are needed during a collection.


Rust provides a convenient way to set the global allocator for a program, and
because \ourgc extends the standard library to include the BDW allocator,
programs can easily be made \ourgc-compliant. This is shown in
\autoref{fig:allocator}.

This ensures that every heap allocation (except those created using a
\lstinline{Gc::new()}) is made using the BDW allocator's
\lstinline{GC_malloc_uncollectable} function. This allocates a
non-garbage-collected block which the collector is aware of and can scan for
pointers to other garbage collected objects. As with the call stack, the BDWGC
scans all allocated blocks in memory that are reachable from the root-set
conservatively word-by-word.

\subsubsection{Pointer obfuscation}

\label{ourgc:pointer_obfuscation}

As a systems programming language, Rust permits operations directly on pointers.
This includes: casting pointers to and from integer types; pointer arithmetic;
and bitwise operations on pointers. All three of these operations can be used to
obfuscate a pointer, hiding it from the collector and causing it to
erroneously determine that an object is unreachable. The user must not obfuscate
any pointers in this way.

\begin{figure}
\begin{lstrustsmall}
fn make_obfuscated() -> usize {
    let a = Gc::new(123_u64);

    // Get a raw pointer to the underlying allocation
    let aptr = a.as_ref() as *const u64;

    // Use the bitwise NOT operator to obfuscate the pointer
    return !(aptr as usize)
}

fn main() {
    let obf = make_obfuscated();

    ...

    // GC cycle here. The `Gc` is potentially unreachable!

    let reify = (!obf) as *const u64;

    // `unsafe` is needed to dereference a raw pointer
    let value = unsafe { *reify };
}
\end{lstrustsmall}
\caption{An example of how pointer obfuscation in Rust can hide a pointer from
    the collector. \ourgc uses the BDWGC to conservatively scan the stack, so
    this allocation could be missed if its only remaining reference is the one
    obfuscated on line 8. Fortunately, however, it requires an unsafe block to
    dereference (line 21).}
\label{lst:obfuscation}
\end{figure}

\subsubsection{Pointer casting and word alignment}

For \ourgc to be able to locate pointers during a marking, all references, raw
pointers, and machine-word sized integers (\lstinline{usize}) must be
word-aligned. This is because Boehm scans the stack and heap blocks for pointers
Word-by-word, so non-word-aligned values may be missed.

It's easy to see that \ourgc needs to be able to identify objects via references
or raw pointers, and thus requires them to be word-aligned. The reason this is
also true for \lstinline{usize}s is more subtle, and is necessary because it's
possible in Rust to cast between raw pointers and word-sized integers using the
\lstinline{as} keyword. Consider the following:

\begin{lstrustsmall}
let gc = Gc::new(Value);
let gc_ref = gc.as_ref(); // Get a &Value reference.
let ptr_to_int = (gc_ref as *const Value) as usize;
\end{lstrustsmall}

We first obtain a reference to the \lstinline{Value} stored inside the
\lstinline{Gc} before casting it to a raw pointer, and then a \lstinline{usize}
(a word-sized unsized integer). If \lstinline{gc} and \lstinline{gc_ref} were to
go out of scope, the \lstinline{ptr_to_int} is enough to keep the GC'd object
alive, because when \ourgc scans the stack, it would correctly identify that
\lstinline{ptr_to_int} looks like a pointer to a valid GC object.

\section{The collector}
\label{ourgc:collector}

\ourgc uses the Boehm-Demers-Weiser GC (\boehm) as the collector implementation
~\citep{boehm88garbage}. \boehm is a conservative mark-sweep collector. It
scans the Rust program's call stack conservatively to look for pointers to
garbage-collected objects when marking. The rest of this section describes the
necessary changes we made to \boehm.

\subsection{Disable parallel collection}

The BDWGC uses parallel collector threads for both mark and sweep phases. We
disable these for performance reasons because we found the performance impact
of the lock contention over heap allocation too high when using concurrent
mutator threads. \jake{TODO: explain}

\subsection{Thread-local storage support}

\boehm does not provide a way for us to scan thread-locals for pointers, se
provide a solution to this for both the POSIX thread-local \lstinline{specific}
API, and thread-locals which use fast compiler generated TLS.
\jake{Realistically, people will only use the latter. So it might not be worth
even mentioning the POSIX stuff}.

By default, Rust uses LLVM's TLS implementation where thread-local data is
stored in the \lstinline{PT_TLS} segment of the ELF binary. This must be considered part of
the root-set during collection. So that \boehm can scan this, we must be able
to locate the \lstinline{PT_TLS} block for each thread when they are suspended during GC,
and then add this to the range of values which need marking. Each thread that
is suspended during a collection is scheduled will call
\lstinline{dl_iterate_phdr(3)} to get the start and end of its own range in the
\lstinline{PT_TLS} segment. These ranges are then scanned during marking. We must do this
at each collection, rather than once at start-up because this segment can grow
and shrink dynamically during the course of the application, and threads can be
spawned or killed in-between collections.

\subsection{Off-thread finalisation API}

We introduces new malloc functions in \boehm for allocating \lstinline{Gc<T>}
objects which require finalising on a separate, dedicated finalisation thread
(\lstinline{GC_buffered_finalize_malloc}, \lstinline{GC_posix_memalign}, \lstinline{GC_memalign}).

These functions allocate objects where the first word in the object points to
their finaliser. (specifically, a fn pointer to \lstinline{drop_in_place} for
\lstinline{T} in \lstinline{Gc<T>}.). This fn pointer must also be tagged in
order to differentiate it from an empty block (as \boehm uses an optimisation
where the first word of empty blocks are used to create a threaded freelist
implementation) \jake{probably unnecessary detail}.

During the sweep phase of a collection, a pointer to each unreachable
finalisable object is added to a \emph{finalisation buffer}. A separate
finalisation thread goes through these objects in the buffer and finalises
them, before deallocating the buffer entirely. This thread is suspended as with
other mutator threads when a GC pause happens, so no synchronisation is needed
between adding objects to the buffer and processing objects already in the
buffer. This thread is spawned lazily depending on whether a finalisable object
exists. If a program contains no finalisable objects, no finalisation thread is
spawned.

\subsection{Parallel mutator threads}

\jake{This needs moving somewhere more sensible as it describes \boehm functionality as-is.}

In order to support thread-safe \lstinline{Gc}s in \ourgc, the BDWGC must be
able to scan each thread's call stack for roots. I extend the Rust compiler to
register newly spawned threads with BDWGC's collector, and to unregister them
when they are destroyed.

\ourgc relies on the BDWGC's signal spin implementation to come to a GC
safepoint. That is, when a mutator thread comes under allocation pressure and
needs to schedule a GC, the BDWGC will send a SIGPWR signal to each registered
thread and has them spin in a signal handler while the collection cycle takes
place.

The main disadvantage of this approach is that it makes use of
implementation-defined behaviour because it relies on the target OS's mechanism
for pausing threads. The BDWGC provides implementations for most platforms, but
it is not portable. An implementation where Rust inserts thread pause safepoints
at appropriate locations would largely solve these issues, though at
the expense of considerable implementation effort.

\section{Finaliser Safety Analysis}

The decision to make \ourgc a conservative GC was relatively simple. A much
more difficult question is: what should we do with drop methods and finalisers?
There are, broadly speaking, three possible design choices: (1) universally
call drop methods from finalisers and accept that this undermines soundness;
(2) require programmers to manually implement a finaliser for each type they
wish to GC and accept the resulting boilerplate; (3) analyse drop methods and
only allow those determined to be safe to be used as finalisers. To the
best of our knowledge, all current GCs for Rust that support finalisers
\laurie{do i remember correctly that some, maybe luster, don't support
finalisers?} \jake{Luster (now gc-arena) does support finalisation \lstinline{https://github.com/kyren/gc-arena/blob/64ab98785417dd8b82737e6c34a80fb6e0f46f87/src/arena.rs#L323}} use the second approach.

\ourgc introduces the novel concept of \emph{Finaliser Safety Analysis} (FSA)
as a way of realising the third approach. FSA extends Rust's type rules to
reject unsafe drop methods when used as finalisers, requiring the user to
either override the check, or to implement a separate finaliser. Since FSA is
designed to be sound (i.e.~without false positives), a practical challenge is
to make it accept enough safe drop methods to be useful. In this section we
explain the motivation for, design, and implementation in \rustc, of FSA.

\laurie{remember to mention auto-traits and type coherence here}

\jake{Expanding here on the limitations of our FSA approach by re-using Drop}
The major limitation of our approach to using \lstinline{Drop} as a GC
finaliser is that it can cause breaking changes for upstream crate authors who
were not aware their types' drop methods were being used downstream inside a
GC. If they make changes to one of their \lstinline{T}'s drop method unaware of
the consequences this has on GC, then a downstream crate may no longer compile
because FSA rejects this. This means that breakages are possible without any
changes to clearly delineated API boundaries such as function signatures. A
separate \lstinline{Finalize} trait would solve this, but at the cost of a lot
of boilerplate code.

\subsection{Which drop methods are safe finalisers?}
\label{finaliser_thread}

A fundamental property of finalisers is that they are run at some unspecified
point in time after the last \lstinline{Gc<T>} reference to a GCed value is
dropped. If we assume a \emph{stop-the-world} GC (i.e.~a GC which periodically
pauses all mutator threads while GC actions are performed) then it may seem logical
to run finalisers during GC pauses. However this causes similar challenges to
Unix signal handlers (where only `async-signal-safe' functions should be run):
if a mutator holds a resource (e.g.~a mutex), and is then interrupted by a
finaliser which tries to acquire that same resource, then the program has
deadlocked. Most GCs, including BDWGC and thus \ourgc, run finalisers on a
separate `finalisers only' thread to avoid this problem.

For it to be safe to run finalisers on a separate thread, those finalisers
must be thread safe.


\subsection{Finaliser safety analysis}
\label{fsa}

\ourgc uses a novel technique to ensure that finalisers are sound called
\emph{Finaliser Safety Analysis} (FSA). The basic idea is to encode finalisation
rules into Rust's type system, and then perform a conservative static analysis
to ensure a type's drop method does not use any types which would be non-thread
safe in a finaliser. \autoref{fig:fsa_example} shows how this can
prevent a potential race condition due to thread-unsafety.

\begin{figure}[t]
% Hack to get the listing in the figure
\newsavebox{\fsaalisting}
\begin{lrbox}{\fsaalisting}
\begin{lstrustsmall}
use std::rc::Rc;
use std::gc::Gc;

fn main() {
    let rc1 = Rc::new(123);
    let rc2 = Rc::clone(&rc1);
    let gc1 = Gc::new(rc2);
}
\end{lstrustsmall}
\end{lrbox}

\newsavebox{\fsablisting}
\begin{lrbox}{\fsablisting}
\begin{lstrustsmallnonums}
error: `rc2` cannot be safely finalized.
    --> src/main.rs:9:22
     |
9    |     let gc = Gc::new(rc2);
     |                      ^^^ has a drop method
     |                          which cannot be
     |                          safely finalized.
     |
    ::: /rust/library/alloc/src/rc.rs:1559:13
     |
1559 |             self.inner().dec_strong();
     |             ------------
     |             |
     |             caused by the expression in
     |             `fn drop(&mut)` here because
     |             it uses a type which is not
     |             safe to use in a finalizer.
     |
     = help: `Gc` runs finalizers on a separate
     | thread, so drop methods must only use values
     | whose types implement `FinalizerSafe`.

\end{lstrustsmallnonums}
\end{lrbox}
    \centering
    \subfloat{\usebox{\fsaalisting}}
    % \hfill
    \subfloat{\usebox{\fsablisting}}
    \caption{A \lstinline{Gc} object stores a reference counted integer
(\lstinline{Rc<u64>}). The drop method of an \lstinline{Rc} decrements the
underlying object's reference count, which is called as a finaliser before
collecting the \lstinline{Gc<Rc<u64>>} object. This is not thread-safe because
decrementing a reference count in \lstinline{Rc} is non-atomic, and the
finaliser will be run on a separate thread, which would race if the finaliser
ran while the mutator also updated the count. FSA detects this, and prevent the program from compiling, with the error message
in \textbf{(b)} displaying the exact line of code that FSA rejected. In this
example, as is often the case in practise, the offending line is not \emph{in}
the finaliser but is \emph{called} by the finaliser.
}
\label{fig:fsa_example}
\end{figure}


% \begin{lstrustsmall}
% use std::rc::Rc;
% use std::gc::Gc;

% fn main() {
%     let rc1 = Rc::new(123);
%     let rc2 = Rc::clone(&rc1);
%     let gc1 = Gc::new(rc2);
% }
% \end{lstrustsmall}

% A \lstinline{Gc} object stores a reference counted integer
% (\lstinline{Rc<u64>}). The drop method of an \lstinline{Rc} decrements the
% underlying object's reference count, which is called as a finaliser before
% collecting the \lstinline{Gc<Rc<u64>>} object. This is not thread-safe because
% decrementing a reference count in \lstinline{Rc} is non-atomic, and the
% finaliser will be run on a separate thread, which would race if the finaliser
% ran while the mutator also updated the count.

% FSA detects this, and prevent the program from compiling, with the error message
% in \textbf{(b)} displaying the exact line of code that FSA rejected. In this
% example, as is often the case in practise, the offending line is not \emph{in}
% the finaliser but is \emph{called} by the finaliser.


FSA permits a \lstinline{Gc} to contain values with non-thread-safe fields
provided they are not used in the drop method. Consider the following example:

\begin{lstrustsmall}
struct Wrapper(RefCell<String>);

impl Drop for Wrapper {
    fn drop(&mut self) {
        println!("Dropping Wrapper");
    }
}

impl Wrapper {
    fn swap_string(&self) {
        *self.0.borrow_mut() = String::from("b");
    }
}

fn main() {
    let gc = Gc::new(Wrapper(RefCell::new(String::from("a"))));
    gc.swap_string();
}
\end{lstrustsmall}

The \lstinline{Wrapper} uses a \lstinline{RefCell} to swap the value of
underlying string (line 11). A \lstinline{RefCell} provides a form of interior
mutability which is not thread-safe (because its \lstinline{borrow()} /
\lstinline{borrow_mut()} methods are non-atomic). In this example, a
\lstinline{Wrapper} can safely be placed inside a \lstinline{Gc}, because the
\lstinline{RefCell} is not used in \lstinline{Wrapper}'s finaliser (line 4).
This is checked by FSA at compile-time.

\subsubsection{Automating finaliser safety analysis}

Finaliser safety analysis is performed automatically without needing to do
anything manually. First, I introduce a new auto trait used as a marker for
finaliser safety called \lstinline{FinalizerSafe} (an introduction to auto
traits is provided in \autoref{sendsync}). As an auto trait, \lstinline{FinaliserSafe} is
implemented for all types by default in Rust, so in the Rust standard library, I
explicitly remove the implementation of \lstinline{FinalizerSafe} from types
which do not already implement \lstinline{Send} and \lstinline{Sync}.

The Rust compiler is then extended to perform FSA. The basic idea is that
whenever a type is used in a \lstinline{Gc}, that type's drop method needs to be
checked to ensure it doesn't access a field which is not
\lstinline{FinalizerSafe}. Performing this check only when such types are used
in \lstinline{Gc} is important as it prevents FSA from breaking existing Rust
programs: drop methods with unsound finalisation behaviour are not a problem if
they are never used in a \lstinline{Gc}.


\subsubsection{Implementation}

The Rust compiler pipeline lowers a Rust program into various different
intermediate representations (IR) which it performs its analysis on. FSA is
performed on Rust's mid-level IR (MIR), which represents a control-flow graph of
a Rust program. MIR represents a Rust program as a collection of \emph{MIR
bodies}, which map to a single Rust function. A \emph{MIR body} consists of a
set of \emph{basic blocks} connected by edges known as \emph{terminators}. Basic
blocks represent a list of straight-line statements, where terminators represent
the control flow in the program.

FSA is a flow-sensitive analysis, so MIR is the most natural representation of a
Rust program to perform its analysis on. FSA is implemented as a new \emph{MIR
pass} -- a traversal over the MIR where each MIR body can be individually
processed. We describe the algorithm for FSA in stages using pseudocode as
follows.

The first stage of FSA is to identify calls to \lstinline{Gc::new}\footnote{In
\ourgc, a \lstinline{Gc} object can only be created through the
\lstinline{Gc::new} constructor. We mark the definition of this function with a
special label, known as a \emph{diagnostic label}, so that it can be easily
referred to during the FSA phase of Rust compilation.}:

\begin{algorithm}
\begin{algorithmic}
\Function{FinaliserSafetyAnalysis}{$prog$}
    \ForEach {$mir\_body \in prog$}
        \ForEach {$basic\_blocks \in mir\_body$}
            \ForEach {$block \in basic\_blocks$}
                \If {\Call{IsCallToGcConstructor}{$block.terminator$}}
                    \State \Call{CheckCallsiteForDropImpl}{$block.terminator$}
                \EndIf
            \EndFor
        \EndFor
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

% \begin{lstlisting}
% DropSafetyAnalysis(prog):
%     for mir_body in prog:
%         for basic_blocks in mir_body:
%             for statement in basic_blocks
%                 where statement is Call:
%                     if statement.call_id == GC_CTOR
%                         CheckCallsite(statement);
% \end{lstlisting}

This checks every statement in the MIR for a call to \lstinline{Gc<T>::new}
constructor. If found, we check if \lstinline{T} implements \lstinline{Drop}. If
it does, then \lstinline{Gc<T>} needs finalising, so the MIR body for
\lstinline{T}'s drop method is checked for soundness violations. FSA only
considers a drop method sound if fields which are dereferenced implement the
\lstinline{FinalizerSafe} trait. In Rust's MIR terminology,
such a field access would constitute a \emph{place projection}, where a place is
a memory location (or lvalue), and a projection is a field access. A single MIR
statement can contain more than one field projection
(e.g.~\lstinline{self.foo.bar.baz}).

This part of the analysis happens in the \lstinline{CheckCallsite} function:

% \begin{lstlisting}
% CheckCallsite(call_site):
%     assert call_site.args.length() == 1
%     let arg_ty = GetType(call_site.args.first())
%     let drop_body = GetDropMirBody(arg_ty)
%     if !drop_body:
%         return
%     for basic_blocks in drop_body:
%         for statement in basic_blocks:
%             if statement == PlaceProjection:
%                 for projection in statement:
%                     CheckProjection(statement.projection)
% \end{lstlisting}

\begin{algorithm}
\begin{algorithmic}
\Function{CheckCallsiteForDropImpl}{$callsite$}
    % \If {\Call{NumArgs}{$callsite$} != 1}
    \State $arg\_ty \gets$ \Call{GetTypeOfFirstArg}{$callsite$}
    \If { \NOT \Call{Impls}{$arg\_ty$, $Drop$}}
        \State \Return
    \EndIf
    \State $drop\_body \gets$ \Call{GetDropMirBody}{$arg\_ty$}
    \ForEach {$basic\_blocks \in drop\_body$}
        \ForEach {$block \in basic\_blocks$}
            \ForEach {$statement \in block$}
                \If {\Call{HasPlaceProjection}{$statement$}}
                    \ForEach {$projection \in statement$}
                        \State \Call{CheckProjection}{$projection$}
                    \EndFor
                \EndIf
            \EndFor
        \EndFor
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

If a projection is found, its type is checked for an implementation of the
\lstinline{FinalizerSafe} trait:

\begin{algorithm}
\begin{algorithmic}
\Function{CheckProjection}{$projection$}
    \Comment{A projection elem is the RHS of a field access.}
    % \If {\Call{NumArgs}{$callsite$} != 1}
    \State $projection\_ty \gets$ \Call{GetType}{$projection.elem$}
    \If{ \NOT \Call{Impls}{$arg\_ty$, $FinalizerSafe$}}
        \State \Return{\Call{Error}{}}
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
% \begin{lstlisting}
% CheckProjection(projection)
%     // A projection element is the RHS of a field access. E.g. `foo.bar`
%     let projection_ty = GetType(projection.elem)
%     if not TypeImplementsTrait(projection_ty, FinalizerSafe):
%         throw error

% \end{lstlisting}

If \lstinline{CheckProjection} discovers a field access of a field which does
not implement \lstinline{FinalizerSafe}, it will throw a compiler error. This
does not halt the analysis, so multiple lines in drop methods which perform
unsound field accesses will be caught in a single FSA pass.

% What is MIR?
% How does it identify call sites
% MIR pass
% Place projection
% Thread_local fix

\subsubsection{Limitations}

As with any static analysis, FSA is inherently conservative: some drop methods
are impossible to analyse at compile-time so in these cases \ourgc will err on
the side of caution and reject potentially safe programs. This is most likely
to happen in two situations.

First, a drop method may contain a call to an opaque (i.e. externally linked)
function for which the compiler does not have the MIR. If a reference to a field
which would be unsafe to use in a finaliser is passed to this function, then FSA
would reject the program. This function call \emph{could} be safe, but FSA has
no way of knowing, and will reject it.

Second, if a finaliser is used on a trait object which is called using dynamic
dispatch in Rust. At compile-time, it is not possible to know the concrete type
of a trait object, so FSA does not know which drop method to check.

In both cases, the user has the option of explicitly informing the compiler a
particular drop method is safe to use as a finaliser. We observe that this is
rare in practice.

\begin{figure}[!t]
\begin{lstrustsmall}
struct Wrapper<'a>(&'a u64);

impl<'a> Drop for Wrapper<'a> {
    fn drop(&mut self) {
        println!("Dropping {}", self.0);
    }
}

fn main() {
    {
        let b = Box::new(123);
        let gc1 = Gc::new(Wrapper(b.as_ref()));
    } // b is dropped

    // GC happens here, calling `Wrapper::drop` as a finaliser.
    // This causes a use-after-free.
}
\end{lstrustsmall}
    \caption{The \lstinline{Gc} now stores a reference to the boxed
    \lstinline{u64} using an intermediate \lstinline{Wrapper} struct which has a
    drop method which will be run as a finaliser (line 4). The boxed
    \lstinline{u64}, and the \lstinline{Gc} are created in an inner scope. At
    the end of this scope, \lstinline{b} is dropped. Later, when the
    \lstinline{Gc<Wrapper>} is collected (e.g.~at line 16) a use-after-free will
    occur because its finaliser (line 4) dereferences a reference to a value
    which was already dropped (at line 14).}
    \label{lst:refwrapper}
\end{figure}

\section{Finalisers and Rust references}

Like other smart pointers in Rust, \ourgc's \lstinline{Gc} container can store
ordinary Rust references. Without finalisers, this is
perfectly safe, because Rust's borrow rules prevent the references from
outliving their referent. Consider the following Rust program:

\begin{lstrustsmall}
fn main() {
    let b = Box::new(123);
    let gc1: Gc<&u64> = Gc::new(b.as_ref());

    foo(b);
    // println!("Boxed value: {}" gc1); // ERROR: use-after-move
}

fn foo(b: Box<u64>) {}
\end{lstrustsmall}

A \lstinline{u64} is boxed on the heap, and an immutable reference to it is
stored in a \lstinline{Gc}. This is valid as long as \lstinline{gc1} does not
outlive \lstinline{b}. If line 6 is uncommented, this program would not compile,
because Rust identifies that printing the \lstinline{gc1} would try to
dereference the boxed \lstinline{u64} after it has been moved (line 5). This
would seem to suggest that Rust's borrow rules are enough to allow references to
be used inside \lstinline{Gc} soundly.

Unfortunately, using references stored in a \lstinline{Gc} from a finaliser can
be unsound. The collector is free to schedule a finaliser to run at any point
after a garbage collected object is unreachable, which could mean running after
a value it references is dropped. \autoref{lst:refwrapper} shows how a program
which uses a Rust reference can cause a use-after-free.


To make matters worse, even references reachable from other shared ownership
types are unsafe to access from a finaliser. Consider a slightly modified
example in \autoref{arcbor}, where instead of storing a \lstinline{&u64}
reference directly, the \lstinline{Gc} contains an atomic reference counted
field (\lstinline{Arc}), which stores a \lstinline{&u64} reference.

\begin{figure}
\begin{lstrustsmall}
use std::gc::Gc;
use std::sync::Arc;

struct Wrapper<'a>(Arc<&'a u64>);

impl<'a> Drop for Wrapper<'a> {
    fn drop(&mut self) {
        println!("Dropping {}", self.0);
    }
}

fn main() {

    {
        let val = 0;
        let a = Arc::new(&val);                    // Arc ref count = 1
        let gc = Gc::new(Wrapper(Arc::clone(&a))); // Arc ref count = 2
    } // `a` dropped, Arc ref count = 1

    // GC happens here, calling `Wrapper::drop` as a finaliser.
    // This is unsound, because `Wrapper`'s `Arc` keeps the `&'a u64` alive,
    // but not the data it points to.

}
\end{lstrustsmall}
    \caption{A reference to \lstinline{val} is stored in an \lstinline{Arc},
    which is then shared inside a \lstinline{Gc}. While both the \lstinline{Gc}
    and \lstinline{Arc} go out of scope at the same time as the referent
    \lstinline{val}, the finaliser for \lstinline{Wrapper} can be called at some
    pointer later, each in lines commented below (20-22), causing a dangling pointer dereference when \lstinline{&u64}
    is invalid.}
    \label{arcbor}
\end{figure}


\subsection{Preventing dangling references with the \bor}

\label{sec:bor}

For a \lstinline{Gc<T>} with a finaliser to be sound, it cannot be used with an
object which contains ordinary Rust references. To guarantee this, \ourgc imposes a
restriction called the \emph{\bor}. This rule states that that a type
\lstinline{T} cannot be placed in a \lstinline{Gc} if both of the
following definitions are true:

\begin{definition}
    \lstinline{T} (or any component type of \lstinline{T}) is of some type
        \lstinline{&U} or \lstinline{&mut U}.
\end{definition}
\begin{definition}

    \lstinline{T} has a finaliser. In other words, \lstinline{T} (or
      any component type of \lstinline{T}) has a drop method.
\end{definition}

As with references, this same unsoundness can be caused by storing raw pointers
inside a \lstinline{Gc} (either immutable \lstinline{*const} or mutable
\lstinline{*mut}) and then dereferencing them in a finaliser. However, \ourgc
is sound even without enforcing this rule for raw pointers because dereferencing raw pointers
is already not possible in safe Rust code, and programmers must ensure the
reference is valid for each dereference anyway. \autoref{badbor} shows an example of
some types which do not adhere to the \bor.


\begin{figure}
\begin{lstrustsmall}
struct S<'a> {
    a: &'a str
}

struct T<'a> {
    a: u64,
    b: S<'a>,
}

enum U<'a> {
    A(u64),
    B(String),
    C(S<'a>)
}
\end{lstrustsmall}
    \caption{We assume each definition has a drop implementation. Values of the
    first struct \lstinline{S} cannot be passed to a \lstinline{Gc} because its
    field, \lstinline{a} contains an immutable reference to a string literal
    (\lstinline{&str}). The struct \lstinline{T} cannot be passed to a
    \lstinline{Gc} either because it contains a transitive reference, through
    field \lstinline{b}.\\\\ Finally, an object of type \lstinline{Gc<U>} is not
    possible because the \lstinline{U::C} variant contains a reference. The
    active variant cannot be known statically, so \ourgc disallows it entirely.}
    \label{badbor}
\end{figure}

\subsubsection{Automating the \bor}

\ourgc checks that programs adhere to the \bor at compile-time, throwing an
error for those programs which violate it. Attempting to compile the earlier
example would result in the following error message:

\begin{lstlisting}
error: `Wrapper(b.as_ref())` cannot be safely constructed.
  --> src/main.rs:19:26
   |
19 |         let gc = Gc::new(Wrapper(b.as_ref()));
   |                  --------^^^^^^^^^^^^^^^^^^^-
   |                  |       |
   |                  |       contains a reference (&) which may no longer 
                              be valid when it is finalized.
   |                  `Gc::new` requires that a type is reference free.

warning: `ref_soundness` (bin "ref_soundness") generated 1 warning
\end{lstlisting}

\begin{figure}[t!]
\begin{lstrustsmall}
struct Wrapper<'a>(&'a u64);

impl<'a> Drop for Wrapper<'a> {
    fn drop(&mut self) {
        println!("Dropping Wrapper");
    }
}

fn main() {
    {
        let a = 123;
        let gc = Gc::new(Wrapper(&a));
    } // a is no longer in scope

    // GC happens here, calling `Wrapper::drop` as a finaliser.
    // `Wrapper`'s drop method does not deref the box pointer,
    // so it does not violate the properties we care about,
    // but it does violate the borrow-or-finalise rule.
}
\end{lstrustsmall}
    \caption{A \lstinline{Gc} is created, storing a reference to \lstinline{a} inside a
\lstinline{Wrapper} whose drop method gives it a finaliser. This does not
compile, since it violates the \bor. However, the drop method for
\lstinline{Wrapper} (line 4) does not use the reference \lstinline{&a} at all,
    which makes it perfectly safe.}
    \label{borsound}
\end{figure}

To check whether a type passed to \lstinline{Gc} contains a reference, \ourgc
defines a marker trait with no methods called \lstinline{ReferenceFree} which is
used by the compiler to indicate that a value of a type does not contain
references.

\lstinline{ReferenceFree} is defined as an \emph{auto trait} (see
\autoref{sendsync}), so it's implemented on every type in Rust by default.
\ourgc then explicitly unimplements \lstinline{ReferenceFree} on all reference
types:

\begin{lstrustsmall}
impl !ReferenceFree for &T {}
impl !ReferenceFree for &mut T {}
\end{lstrustsmall}

This means that if a type contains a \lstinline{&T} or a \lstinline{&mut T} for
any \lstinline{T} in
one of its component types, then it will not implement \lstinline{ReferenceFree}
either.

Whether or not a type implements \lstinline{ReferenceFree} can then be checked
at compile-time: if \lstinline{T} implements \lstinline{ReferenceFree}, then
it's safe to use inside a \lstinline{Gc}. If \lstinline{T} does not implement
\lstinline{ReferenceFree}, and \lstinline{T} has a drop method, then it cannot
be used inside a \lstinline{Gc}. This is implemented with an extension to
finaliser safety analysis, where the type of \lstinline{T} is checked for an
implementation of \lstinline{ReferenceFree} if it has a finaliser during the
\lstinline{CheckCallsite} phase.

\subsubsection{Limitations}

The \bor can be restrictive: in particular, a
\lstinline{Gc<T>} object which both contains references and has a finaliser, but
where the finaliser never uses the reference would be deemed invalid, even
though it is sound. \autoref{borsound} shows an example program which is sound, but fails to
compile due to the \bor.

To allow the program in \autoref{borsound} to compile, one can explicitly
implement \lstinline{ReferenceFree} on \lstinline{Wrapper}:

\begin{lstrustsmall}
unsafe impl<'a> ReferenceFree for Wrapper<'a> {}
\end{lstrustsmall}

This will now compile and \lstinline{Gc<Wrapper>} will be finalised before
collection. However, it is unsafe as the onus is now on the programmer to ensure
\lstinline{Wrapper}'s drop method never uses \lstinline{&a}.


\subsection{Finaliser ordering}
\label{bg:finaliser_order}

A language implementation has a choice as to whether to guarantee that
finalisers will run in a certain order. Some GCs implementations guarantee a
finalisation order, because for some applications, this is important if one
resource must be cleaned up before another.

For example, consider the objects in~\autoref{fig:ordered_finalisation}. The
finalisers for the ``outer'' objects all reference the object in the layer
to the right of them, so they must be finalised ``outside-in''.

However, this guaranteed finalisation order has two disadvantages. First,
finalising all the objects in a chain of floating garbage happens over multiple
finalisation cycles because an object can only be finalised if it is not reachable
from other unreachable objects. In~\autoref{fig:ordered_finalisation}, this
would require four finalisation cycles before the floating garbage is reclaimed.
Such a delay in the eventual reclamation of objects can cause \emph{heap drag},
where unreachable objects are kept alive longer than necessary.

Second, and most significantly, this approach is not able to finalise cycles of
objects where more than one object needs finalising, which can lead to resource
leaks. This is because the collector cannot know which (if any) object is safe
to finalise first: if an object references another object which has already been
finalised, this is unsound. Boehm proposes a workaround for this where
programmers can refactor the objects in order to break the finalisation
cycle~\citep{boehm03destructors} (\autoref{fig:finaliser_cycles} from
\citet[p.~218]{jones16garbage}). Another workaround is to allow users to use weak
references to break cycles (similar to breaking reference count cycles).
Unfortunately this is often difficult to implement
correctly~\citep{jones16garbage}.

Some GCs such as Blink's Oilpan GC~\citep{ager13oilpan} do not specify a finalisation order. This
permits the GC to finalise cycles of objects where more than one object needs
finalising with an important restriction placed on the programmer: an object's finaliser must not reference
any other object. This caveat is necessary to prevent dereferencing an already
finalised object. This approach is used in V8: finalisers are not able to
dereference through to other JavaScript objects. In the rare cases where this is
too restrictive, Oilpan also has pre-finalisers, which are run during the GC's
stop-the-world pause, and allow object fields to be
dereferenced~\citep{ager13oilpan}. The compromise here is performance:
pre-finalisers increase the mutator pause time and cannot be enqueued to run
later.

\subsection{Finalisers and deadlocks}
\label{bg:synchronisation}

It is common in many languages for finalisers to access fields from other
objects or even global state. Since an object's finaliser is run at some unknown
point in time once it is considered unreachable by the collector, it must be
able to safely access such state without racing with the mutator.

\begin{figure}[t!]
\begin{lstlisting}
class X {
    Count count;

    @Override
    protected void finalize() {
        count.increment();
    }

    public static void main(String[] args) {
        Count count = new Count();
        X x = new X(count);

        ...

        count.increment();
}

class Count {
    Mutex mutex = new Mutex();
    int count = 0;

    public void increment() {
        try {
            mutex.acquire();
            count++;
        }
        catch (InterruptedException e) {
            e.printStackTrace();
        } finally {
           mutex.release();
        }
    }
}
\end{lstlisting}
    \caption{An example Java program showing how a finaliser could deadlock if
    run from the same thread as the mutator. The \lstinline{Count} object has a
    method \lstinline{increment} which acquires a mutex (line 22). A new
    instance of \lstinline{Count} is created and a reference to it
    passed to a new instance of \lstinline{X} (lines 10-11). If the collector
    decided to invoke \lstinline{X.finalize()} while the mutator executes
    \lstinline{count.increment()} on line 15, then the program will deadlock as
    they both try to acquire the mutex on the same thread. This can be solved by
    ensuring that finalisers run on a separate thread to the mutator: the
    finaliser thread would simply spin or sleep while it waits for the mutator
    to release the mutex.}
    \label{java_finaliser_deadlock}
\end{figure}

It is common in many languages for finalisers to access fields from other
objects or even global state. Since an object's finaliser is run at some unknown
point in time once it is considered unreachable by the collector, it must be
able to safely access such state without racing with the mutator. If a finaliser
runs on a separate thread to the mutator, it's clear that data accessed from a
finaliser must be performed in a way which is thread-safe. However, what's less
obvious is that for correctness reasons, finalisers cannot be scheduled to run
on the mutator thread.

\autoref{java_finaliser_deadlock} shows how finalisers run on the same thread as
the mutator can cause a deadlock. When the finaliser for an instance of
\lstinline{Counter} is called, it tries to acquire the lock for the object's
\lstinline{count} member. If this happens while the mutator already holds the
lock, the program will deadlock. While one might consider using re-entrant locks
to solve this problem, it can in fact worsen the problem by turning an obvious
failure into a race where two logically separate operations mutate data in a way
which should be performed atomically. This can cause intermittent incorrect
execution~\citep{boehm03destructors}.

For this reason, most languages run finalisers on a separate thread from the
mutator~\citep{boehm03destructors}. Though programmers must still ensure they access
shared state via some form of synchronisation such as mutexes, they are able to
spin or sleep when blocked, allowing the mutator to progress without
deadlocking.


\section{Early Finaliser Prevention}
\label{bg:early_finalisation}

Unlike RAII-like destructors found in languages such as C++ and Rust, finalisers
are called by a garbage collector non-deterministically. They can run at the
collector's leisure; often this means that they run later than desired, however,
in rare cases, an object can be finalised while it is still being used by the
mutator! This is because compiler optimisations -- unaware of the presence of a
GC -- can remove the single reference to an object which is keeping it alive. An
outer object can therefore be considered unreachable while its inner object is
still in use. An unfortunately timed GC cycle could  end up finalising the outer
object, and run its finaliser. This can lead to subtle races in programs where
the finaliser interleaves execution with the mutator.

For this reason, VM specifications do not commit to running finalisers at a
specific time. This includes allowing an object's finaliser to be run while the
mutator is potentially still using it. For the reasons outlined in
\autoref{bg:synchronisation}, GC implementations must synchronise access to
objects inside a finaliser. \citet[p~.218]{jones16garbage} suggests this can be
used to defer finalisation until later if a finaliser attempts to acquire an
object lock which is already held by the mutator.

More generally, however, the fundamental problem is that the compiler optimises
away the reference to the object too soon. C\#'s .NET runtime
provides a \lstinline{gc.KeepAlive} function as a solution to this.
\lstinline{gc.KeepAlive} is an opaque empty function which the compiler cannot
optimise away. The idea is that a reference to an object can be passed to
\lstinline{gc.KeepAlive}, ensuring it lives long enough so that the collector
does not deem it unreachable and finalise it too soon. This mitigation is
limited, however, as it is up to the user to call \lstinline{gc.KeepAlive} when
they require it.


\section{Rust destructors}

\label{destructors_detailed}

To understand the design decisions that \ourgc makes surrounding finalisation,
some background is needed on Rust's destructors.

Destructors in Rust were briefly introduced in \autoref{bg:basic_destructors} as
a way of running cleanup code when an initialized variable or temporary goes out
of scope. A destructor is run automatically at the end of the scope for values
which implement the \lstinline{Drop} trait. Consider the following example,
which uses a Rust destructor to close a file descriptor:

\begin{lstrustsmall}
struct FileDescriptor {
    fd: u64
}

impl Drop for FileDescriptor {
    fn drop(&mut self) {
        self.close();
    }
}

fn main() {
    let f = FileDescriptor { fd: 1 };
} // FileDescriptor::drop called.
\end{lstrustsmall}

The file descriptor \lstinline{f} is destructed (or \emph{dropped}) at the end
of the \lstinline{main} function, where it is no longer in scope. The ability to
drop objects is a key component of Rust's ownership semantics, and is used
extensively in the standard library.

A struct which has a drop implementation may have fields which also need
dropping. For example, consider a \lstinline{FileBuffer}, which has a field of
type \lstinline{FileDescriptor}:

\begin{lstrustsmall}
struct FileBuffer {
    descriptor: FileDescriptor,
}

impl Drop for FileBuffer {
    fn drop(&mut self) {
        self.flush();
    }
}
\end{lstrustsmall}

Here, both the \lstinline{FileBuffer} and its field \lstinline{FileDescriptor}
have drop methods which need running. Rust will automatically insert calls
to drop them both when a \lstinline{FileBuffer} value goes out of scope. In Rust
terminology, a value is considered dropped once its drop method, and all drop
methods belonging to its fields, have been dropped.

\subsubsection{Rust drop order}

\label{rust_drop_order}

Drop methods are used by Rust programmers for situations such as releasing
locks. In such cases, the order in which values are dropped is vital for
program correctness.

Variables and temporaries are dropped in reverse declaration order. For example:

\begin{lstrustsmall}
fn main() {
    let s1 = String::from("s1");
    let s2 = String::from("s2");
    let s3 = String::from("s3");
}
\end{lstrustsmall}

At the end of \lstinline{main}, \lstinline{s3} would be dropped first, followed
by \lstinline{s2}, and finally \lstinline{s1}.

Rust specifies that fields are dropped in declaration order. For example,
consider the following struct definition:

\begin{lstrustsmall}
impl Drop for S {
    fn drop(&mut self) {
        println!("Dropping S");
    }
}

struct S  {
    a: String,
    b: u64, // u64 does not implement the `Drop` trait.
    c: Vec<bool>,
}
\end{lstrustsmall}

\lstinline{S} contains two fields (\lstinline{a} and \lstinline{c})
which also need dropping. Rust will drop \lstinline{S} first, followed by
the field \lstinline{a}, and then the field \lstinline{c}.

If any component of a type implements \lstinline{Drop}, Rust will drop them when
they go out of scope. For example, consider an enum \lstinline{E} (a tagged
union), where one variant needs dropping:

\begin{lstrustsmall}
enum E  {
    A(String),
    B(bool),
}
\end{lstrustsmall}

Even though \lstinline{E} does not have a drop method, when it goes out of scope,
Rust will still insert a drop call because the variant \lstinline{E::A} contains
a droppable type, \lstinline{String}. It's not possible to know at runtime which
variant of the enum is active, so Rust inserts some additional code which checks
dynamically which variant (if any) to drop.


Since Rust ensures that drop methods are called automatically, it is not
possible to call the drop method for a value (or any of its fields) directly.
This ensures that a value is only dropped once, an important protection against
double-freeing resources. This can be restrictive, because sometimes it's useful
to drop a value earlier than at the end of its scope. Consider a common example,
where a \lstinline{Mutex}'s lock is released from its drop method:

\begin{lstrustsmall}
fn main() {
    let mutex = Mutex::new(123); // A mutex which guards a u64 value.
    let data = mutex.lock().unwrap();
    println("locked value: {}", data);

    // Code that shouldn't belong in the critical section
    ...
} // lock is released as part of drop.
\end{lstrustsmall}

By unlocking the mutex at the end of main, sometimes we can execute more code
than is necessary in the critical section. Rust provides a standard library
helper function, \lstinline{std::mem::drop} which can accept values of any type
in order to drop them early:

\begin{lstrustsmall}
fn main() {
    let mutex = Mutex::new(123); // A mutex which guards a u64 value.
    let data = mutex.lock().unwrap();
    println("locked value: {}", data);

    // Release the lock early
    std::mem::drop(data);

    // Code that shouldn't belong in the critical section
    ...
} // lock is released as part of drop.
\end{lstrustsmall}

\lstinline{std::main::drop} is implemented as an empty function. Since ownership
of \lstinline{data} is transferred (line 7), Rust will insert a call to
\lstinline{data}'s drop method immediately afterwards.

\subsubsection{Drop methods are not guaranteed to run}

Destructors in Rust are guaranteed to run at most once --- but they may not be
run at all. This is for three reasons.

First, consider the example back in \autoref{ch:rust} (\autoref{rc_cycle_drop}) where a cycle is created
between two \lstinline{Rc} values. A reference cycle such as this introduces a
memory leak and thus the values in this data structure are never dropped.

Second, values are only dropped if they are initialized. It is not always
possible to know whether a value is initialised so Rust can sometimes end up
performing dynamic checks to know whether a value should be dropped. The details
of this are not relevant to the rest of the thesis.

Third, one can explicitly prevent a value from being dropped by passing it to
the \lstinline{std::mem::forget<T>} function. This is commonly used when the
underlying resource originated from non-Rust code, and therefore destruction of
it should happen outside of Rust.

\subsubsection{What types can be dropped?}

\label{bg:drop_what_types}

In short, Rust will automatically call drop for any type which implements the
\lstinline{Drop} trait when it goes out of scope. However, copyable types
(those which implement the \lstinline{Copy} trait, mostly primitive
types such as \lstinline{bool}s, \lstinline{char}s, numeric types and so on)
cannot implement Drop because doing so would mean that
when values are copied they would be dropped multiple times. This
would violate Rust's guarantee that drop is called at most once.

Rust also supports C-like union types, which in contrast to enums do not use
runtime tags to denote the active variant. Union types are not automatically
dropped because there is no way for Rust to know which variant to insert a drop
method for.


\section{Design choices for finalisers in Rust}

Before explaining finalisation in \ourgc, we should ask an over-arching design
question: what should a finaliser in a Rust GC look like? Other approaches to GC
in Rust, such as \rustgc and \bronze define a custom \lstinline{Finalize} trait,
which types can implement to specify finaliser behaviour when they are used in a
\lstinline{Gc} (shown in \autoref{lst:finalise_trait}).

\begin{figure}[t!]
\begin{lstrustsmall}
struct S;

impl Drop for S {
    fn drop(&mut self) {
        println!("Dropping S");
    }
}

impl Finalize for S {
    // Run before collection when value used in a `Gc`.
    fn finalize(&mut self) {
        println!("Finalizing S");
    }
}

fn main() {
    let s1 = S;
    let s2 = S;

    let gc1 = Gc::new(s2);
} // Dropping s1
\end{lstrustsmall}
    \caption{An example from \rustgc, where a custom \lstinline{Finalize} trait
    is used for finalisation semantics. In this scenario, before \lstinline{s2}
    is collected, \rustgc calls \lstinline{S}'s \lstinline{finalize} method
    (line 11).}
    \label{lst:finalise_trait}
\end{figure}


The benefit of this approach is that it creates a logical separation between
destructors expected to run in an RAII based context, and GC finalisers. This
allows finalisers, which have subtly different rules to destructors, to be
correctly specified by the user (as we will see in
\autoref{ourgc:threadsafe_finalisation}, there are specific restrictions that need to be placed on
finalisers in Rust in order to guarantee soundness).

\ourgc takes a different approach, however, as separating destruction and
finalisation in this way has unfortunate consequences. First, for most types
that already implement Drop, their destruction logic must be duplicated in a
finaliser. This is, at least, significant extra effort; it also offers many
opportunities for copy and paste errors.

Second, a separate \lstinline{Finalize} trait has as a major ergonomic cost
because it's not possible to implement \lstinline{Finalize} on code from
external libraries. This is because Rust enforces \emph{trait coherence}, a
property in the language which ensures that every type has at most one
implementation of a given trait. This coherence rule is fundamental to the
language, because it removes ambiguity in trait method resolution, ensuring
there is only one implementation of a trait method to choose from.

Trait coherence is a problem for programs that use external compilation
units known as \emph{crates} (roughly speaking, `libraries'), because if two
unrelated crates provide separate implementations for the same trait, then those
crates cannot be imported together.
\begin{figure}[t!]
% Hack to get the listing in the figure
\newsavebox{\orphanalisting}
\begin{lrbox}{\orphanalisting}
\begin{lstrustsmall}
use a::{MyType, MyTrait};

impl MyTrait for MyType {
    fn method1() {
        ...
    }

    fn method2() {
        ...
    }
}

\end{lstrustsmall}
\end{lrbox}

\newsavebox{\orphanblisting}
\begin{lrbox}{\orphanblisting}
\begin{lstrustsmall}
error[E0117]: only traits defined in the current
              crate can be implemented for types
              defined outside of the crate
 --> src/lib.rs:3:1
3 | impl MyTrait for MyType {}
  | ^^^^^^^^^^^^^^^^^------
  | |                |
  | |                `MyType` is not defined in
  | |                the current crate
  | impl doesn't use only types from inside the
  | current crate

\end{lstrustsmall}
\end{lrbox}
    \centering
    \subfloat[\centering Invalid trait implementations]{\usebox{\orphanalisting}}
    \hfill
    \subfloat[\centering Compiler error]{\usebox{\orphanblisting}}
    \caption{
        Here, we try to provide an implementation of the externally defined
        trait, \lstinline{MyTrait} for the externally defined type,
        \lstinline{MyType}. This results in a compile error in Rust because it
        violates the orphan rule.}
    \label{lst:coherence}
\end{figure}

To address this, traits in Rust must adhere to something called the \emph{orphan
rule}. The rule is simple: it is not possible to implement a trait for a type
where both the trait and the type are defined in separate crates. This prevents
multiple conflicting trait implementations from existing across crates.
\autoref{lst:coherence} shows how the orphan rule is enforced at compile-time in
Rust.

The problem with the orphan rule is that it would become a major source of
ergonomic frustration for \ourgc if it defined a separate \lstinline{Finalize}
trait. It would not be possible to implement \lstinline{Finalize} for any type
which was not defined in the user's current crate. If types from external crates
do not provide their own implementations for \lstinline{Finalize}, then those
types may cause resource leaks when used in a \lstinline{Gc}.

A workaround for the orphan rule is to use the \emph{new-type idiom}, where the
current crate defines a wrapper type for an external type. Unfortunately, this
workaround can be cumbersome to write and makes types in Rust harder to read.
\autoref{lst:orphan_workaround} shows how the new-type idiom can be used to add
a finaliser to a type defined outside of the current crate. This can be used in
other GC designs for Rust which use a separate \lstinline{Finalize} trait such
as \rustgc.

\begin{figure}[t!]
\begin{lstrustsmall}
use a::MyType;

struct Wrapper(MyType);

impl Finalize for Wrapper {
    fn finalize(&mut self) {
        println!("Finalizing MyType via Wrapper");
    }
}

fn main() {
    let a = Gc::new(Wrapper(MyType::new()));
}
\end{lstrustsmall}
\caption{A workaround the orphan rule using the \emph{new type idiom}. Here, a
    new \lstinline{Wrapper} type is defined for which we define a finaliser. To
    garbage collected \lstinline{MyType} objects, one could then use
    \lstinline{Gc<Wrapper>} instead of \lstinline{Gc<MyType>} to ensure that its
    finaliser is called.}
    \label{lst:orphan_workaround}
\end{figure}

\begin{lstrustsmall}
let a = Box::new(String::from("Hello"));
let b = Gc::new(a);
\end{lstrustsmall}

The sole owning reference to the heap allocation \lstinline{Box<String>} is
moved into \lstinline{Gc::new}, which creates a \lstinline{Gc} object containing
the reference to the \lstinline{Box<String>}. This has the following
representation in memory:

\begin{center}
\includegraphics[width=0.75\textwidth]{images/alloy_finaliser_memory}
\end{center}

When the \lstinline{Gc}'s underlying allocation (called GcBox) becomes
unreachable, \ourgc will call its finaliser, which means that \lstinline{drop}
is called on all the component types (in the same way that Rust automatically
calls drop in \autoref{rust_drop_order}). If, for whatever reason, the finaliser
is not run, then the allocations for the \lstinline{Box} and the
\lstinline{String} will leak (i.e. their heap allocation will never be
reclaimed). I thus define a finaliser in \ourgc as calling drop on the contents
of a \lstinline{Gc} (including its field types). Therefore a type
\lstinline{Gc<T>} has a finaliser if type \lstinline{T} needs dropping.

\subsection{Omitting finalisers}

Finalisation is not always desirable. For example, consider a \lstinline{FileDescriptor}
which uses its drop method to close the descriptor:

\begin{lstrustsmall}
struct FileDescriptor {
    fd: u64
}

impl Drop for FileDescriptor {
    fn drop(&mut self) {
        self.close();
    }
}
\end{lstrustsmall}

Here, objects of type \lstinline{Gc<FileDescriptor>} would use a finaliser to
call the \lstinline{FileDescriptor}'s drop method. However, if we were to close
the descriptor in the mutator once we are finished with the object, the
finaliser is no longer necessary:

\begin{lstrustsmall}
let stdout = FileDescriptor { fd: 1 };
let descriptor = Gc::new(stdout);
...
descriptor.close()
\end{lstrustsmall}

To allow for this, \ourgc provides a special wrapper type,
\lstinline{NonFinalizable<T>}, which can be used to create a \lstinline{Gc}
which omits finalisers on an individual basis:

\begin{lstrustsmall}
let descriptor = Gc::new(NonFinalizable::new(stdout));
\end{lstrustsmall}

Here, when the \lstinline{Gc<NonFinalizable<FileDescriptor>>} is collected, it
will not be finalised. The \lstinline{NonFinalizable<T>} type has no additional
storage costs, and at runtime is represented as a bare
\lstinline{FileDescriptor}.

This is only intended to be used in exceptional circumstances where performance
is a concern: It can easily lead to resources leaks if not used carefully.

\section{Safely finalising objects off-thread}
\label{ourgc:threadsafe_finalisation}

In \autoref{bg:synchronisation}, I explained that running finalisers on
the same thread as the mutator can cause deadlocks.
\autoref{fig:rust_finaliser_deadlock} shows how this could happen in a
hypothetical version of \ourgc where finalisers are scheduled to run from the
mutator thread.

As with any GC, \ourgc can finalise objects at its leisure, with no way for the
programmer to know when this will happen. Rust does not prevent users
from writing code which deadlocks, but it does not make the situation worse.
However, if \ourgc were to perform on-thread finalisation, it would open
the possibility of previously non-deadlocking code deadlocking.

One possible solution might seem to be to prohibit finalisers from acquiring
locks, however this can cause race-like bugs because of how finalisers can
interleave asynchronously with the mutator~\citep{niko13destructors}.

\begin{figure}[htbp]
\begin{lstrustsmall}
use std::gc::Gc;
use std::rc::Rc;
use std::sync::Mutex;

struct CounterWrapper {
    value: Rc<Mutex<usize>>
}

impl Drop for CounterWrapper  {
    fn drop(&mut self) {
        let mut count = self.value.lock().unwrap();
        *count += 1;
    }
}

fn main() {
    // Create a reference counted counter object protected by a mutex.
    let counter = Rc::new(Mutex::new(0));

    {
        // Create a new garbage collected object which contains a reference
        // counted pointer to the counter. When this object is collected, the
        // drop methods for `CounterWrapper` and `Rc` will be called as
        // finalizers.
        let gc = Gc::new(CounterWrapper { value: Rc::clone(&counter) });
    }

    // Assume GC can happen here because `gc` is unreachable.

    // If the finalizer for the `Gc<CounterWrapper>` is asynchronously called
    // while `counter.lock()` below is already acquired, a deadlock will occur.
    let lock = counter.lock().unwrap();
    ...

    assert_eq!(*counter, 0);
}
\end{lstrustsmall}
    \caption{An example showing how a
    potential deadlock can be caused if finalisers are run on the mutator
    thread. A shared counter is created using a reference counted container (line 18), a
    reference to this is then placed inside a garbage collected container (line
    25). This is potentially short-lived, as it is only
    reachable by the \lstinline{gc} variable until the end of the inner scope
    (line 26). If \ourgc decides to
    schedule a collection after this (e.g.~on line 28) then the
    \lstinline{CounterWrapper} could be considered garbage, where a finaliser
    would run its drop method (line 10). If this happens while the main mutator
    thread already holds \lstinline{counter.lock()} (line 32-36) then this program
    can deadlock. When finalisers are run on the same thread, there is nothing
    the programmer can do in this situation to guarantee that this kind of
    deadlock does not occur.}
    \label{fig:rust_finaliser_deadlock}
\end{figure}

\subsection{Off-thread finalisation}

\label{alloy_off_thread}

\ourgc finalises unreachable objects on a separate finalisation thread.
This means that finalisers cannot deadlock simply by acquiring a
lock held by the mutator: the finalisation thread will simply wait and attempt
to re-acquire the lock later. However, this now means that shared data, or other
objects accessed from a finaliser must be done in a thread-safe way. The problem
with this is that in \ourgc, a finaliser calls a type's existing drop methods.
Since \lstinline{Drop} was not originally defined in expectation of being
called on a separate thread, it does not guarantee thread safety.

\begin{figure}[t!]
\begin{lstrustsmall}
use std::cell::Cell;
use std::gc::Gc;
use std::thread;

struct Counter(Cell<usize>);

// `Counter` is not `Sync` by default because it contains a `Cell`. However,
// it doesn't have a drop method, so it's safe to use inside a `Gc`. In order to
// do this, we must explicitly mark it as `Sync` so that `Gc` accepts it.
unsafe impl Sync for Counter {}

fn main() {
    let gc = Gc::new(Counter(Cell::new(0)));
    let gc2 = gc;


    // By explicitly implementing `Sync` for `Counter`, we've accidentally
    // allowed it to be used across threads in ways unrelated to finalisers
    // which are not thread-safe. E.g. the following can race, as the counts are
    // not updated atomically.
    thread::spawn(move || {
        for i in 0..1000 {
            gc2.0.set(gc.0.get() + 1);
        }
    });

    for i in 0..1000 {
        gc.0.set(gc.0.get() + 1);
    }
}
\end{lstrustsmall}
    \caption{An example of the \lstinline{Send} + \lstinline{Sync} dilemma if
    \ourgc required \lstinline{T: Send + Sync} for \lstinline{Gc<T>} to provide finaliser
    safety. Such a restriction on \lstinline{T} is overly strict, and could lead
    to users explicitly marking types as \lstinline{Send} or \lstinline{Sync} so
    \ourgc considers them safe for finalisation. This example shows how this can
    lead to inadvertantly bypassing Rust's concurrency safety when such types
    are used elsewhere. By default, \lstinline{Counter} cannot be used inside a \lstinline{Gc} because
    it contains a field \lstinline{Cell} which does not implement
    \lstinline{Sync}. To allow the construction of \lstinline{Gc<Counter>},
    \lstinline{Sync} can be explicitly implemented on \lstinline{Counter} (line
    10). However, this now allows objects of this type to be used across threads
    without any synchronisation (lines 21-29), which can cause surprising races.}
    \label{send_sync_dilemma}
\end{figure}

The most obvious solution to this is to ensure that only thread-safe types
can be used inside a garbage collected container. In other words, a type
\lstinline{T} could not be placed inside a \lstinline{Gc} unless \lstinline{T}
implements both \lstinline{Send} and \lstinline{Sync} -- Rust's builtin traits
for concurrency safety (see \autoref{sendsync}). This solution would
prevent programs from compiling if an object without synchronisation is placed
inside a \lstinline{Gc} container. While this would ensure that finalisers are
thread-safe, it is less than ideal for two reasons.

First, it would restrict a \lstinline{Gc} from managing many valid types: a
non-\lstinline{Send} and non-\lstinline{Sync} type would be prevented from being
used in a \lstinline{Gc} even if it doesn't have a drop method (and therefore,
never needed finalising in the first place!).

Second, for \lstinline{T} to be \lstinline{Send} and \lstinline{Sync}, all of
\lstinline{T}'s component types must be \lstinline{Send} and \lstinline{Sync}
too. This presents a dilemma: either every field of \lstinline{T} must be
thread-safe (even those which are never used in a finaliser); or, the user,
certain in the knowledge that \lstinline{T}'s drop method is thread-safe,
forcibly \lstinline{unsafe} implements \lstinline{Send} and \lstinline{Sync} on \lstinline{T}. In the
case of the latter approach, \lstinline{T} can then be accidentally be used in
concurrency contexts unrelated to garbage collection, bypassing an important
part of the type system in order to keep \ourgc happy.
\autoref{send_sync_dilemma} shows how this could cause a leaky abstraction which
introduces bugs in non-GC related code.


\subsection{Finalisation order}

In \autoref{bg:finaliser_order} I explained how a GC can guarantee ordering on
finalising floating garbage. The main disadvantage of this approach is that
objects with finalisation cycles will not be finalised. The alternative is for
the collector to not specify any order. This allows objects with cycles to be
finalised but with a heavy constraint: they must not reference other objects
from inside their finalisers. For many GCs, this is too restrictive.
\citet{boehm03destructors} makes the case that in languages such as Java, the usefulness
of finalisers depends on them being able to interact with other objects. As
such, it is common to see VMs for managed languages such as Java and .NET
specify an ordering for finalisers.

However, our requirements for GC are unique in this respect: \ourgc is not
intended to replace Rust's RAII approach to memory management, instead, it
provides optional GC for objects where the RAII approach is difficult or
impossible. It is not uncommon to see Rust programs which use \ourgc with a mix
of GC'd and non-GC'd objects. In such cases, it is safe for a finaliser to access
a field of a non-GC'd object because there is no danger of them being finalised
already. \autoref{orderunsound} shows an example of how dereferencing another
\lstinline{Gc} can be unsound, with \autoref{orderunsoundmemory} showing its
representation in memory.

\begin{figure}[t!]
\begin{lstrustsmall}
impl Drop for Node {
    fn drop(&mut self) {
        // println!("Dropping {}", self.a) // Unsound when `Node` used in Gc!
        println!("Dropping {}", self.b)
    }
}

struct Node {
    a: Gc<u64>,
    b: Box<u64>,
}

let gc1 = Gc::new(Node{ a: Gc::new(1), b: Box::new(2) });
\end{lstrustsmall}
    \caption{Here, it's safe for \lstinline{Node}'s finaliser to reference its field
\lstinline{a} (line 4) because it points to an non-GC'd object. In contrast, if
line 3 was uncommented this program would be unsound. Unlike in languages such
as Java, where every object is managed, the restriction of dereferencing through
fields of other GC'd objects is less of an issue because non-GC'd objects can
    still be accessed.}
    \label{orderunsound}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.75\textwidth]{images/alloy_inner_gc}
\end{center}
    \caption{\autoref{orderunsound} representation in memory.}
    \label{orderunsoundmemory}
\end{figure}


In addition, one of \ourgc's main goals is to make it easier to work with data
structures that have cycles in Rust. It is suggested that finalisation cycles
are rare in GC'd languages~\citep{jones16garbage}. However, this is different for
\ourgc, since destructors in Rust are common, and mapping them to finalisers
means that it is not uncommon to see finalisation cycles in Rust programs using
\ourgc.

For these reasons, I decided that by default, \ourgc does not guarantee a
finalisation order. This allows cyclic data managed by \ourgc to be finalised,
preventing potential resource leaks. In \autoref{sound_unordered_finalisation} I
describe how this is made sound by preventing a \lstinline{Gc}'s finaliser from
dereferencing a field to another \lstinline{Gc}.

\subsubsection{Forcing ordered finalisation}

If users find the constraint this imposes on finalisers too
restrictive, they may instead choose to build \ourgc with ordered finalisation.
This can be done with the \lstinline{-DENABLE_TOPOLOGICAL_FINALIZATION} build
flag, which uses the BDWGC's topological finalisation order. In this configuration,
cyclic data structures such as that shown in
\autoref{fig:ordered_finalisation_alloy} would not finalise, leading to a memory
leak.

While ordered finalisation would allow finalisers to dereference
fields to other \lstinline{Gc}'s, they must still only access fields which use
the \lstinline{FinalizerSafe} trait for the soundness reasons explained in
\autoref{alloy_off_thread}.

\begin{figure}[t]
\begin{lstrustsmall}
use std::gc::Gc;
use std::sync::Mutex;

struct Node {
    name: String,
    next: Option<Gc<Mutex<Node>>>,
}

impl Drop for Node {
    fn drop(&mut self) {
        println!("Dropping {}", self.name);
    }
}

fn main() {
    let a = Gc::new(Mutex::new(Node {
        name: String::from("a"),
        next: None,
    }));

    let b = Gc::new(Mutex::new(Node {
        name: String::from("b"),
        next: None,
    }));

    a.lock().unwrap().next.insert(b);
    b.lock().unwrap().next.insert(a);
}
\end{lstrustsmall}
    \caption{An example of a small cyclic graph using \ourgc. This example
    creates two nodes, \lstinline{a} and \lstinline{b}, before creating cyclic
    references between them (lines 26-27). If \ourgc used ordered finalization,
    then neither \lstinline{a} nor \lstinline{b} would be finalised. This would
    cause a memory leak, because \lstinline{Node.name} contains an owned
    \lstinline{String}. A \lstinline{String}'s drop method is used to deallocate
    its backing store, which never be called. \\\\
    \ourgc can therefore only represent this cyclic data structure without
    memory leaks if it runs finalisers without a specified ordering. This way,
    the collector can decide at its leisure whether to finalise \lstinline{a} or
    \lstinline{b} first.}
    \label{fig:ordered_finalisation_alloy}
\end{figure}

\subsubsection{Soundness issues with unordered finalisation}
\label{unordered_finalisation_soundness}

The problem with finalising \lstinline{Gc}s in an unspecified order is that any
other \lstinline{Gc} object which they reference may have already been freed.
Consider the example in \autoref{fig:unsound_finalisation_cycle}, which
allocates \lstinline{Gc}s which reference each other in a cycle. A cycle such as
this one will always lead to unsoundess with finalisers which access other
\lstinline{Gc} objects. This is also a problem for non-cyclic data structures in
\ourgc's non-ordered configuration as there is no guarantee that non-cyclic data
structures will be finalised `outside-in'.

\begin{figure}[t]
\begin{lstrustsmall}
struct Node {
    name: String,
    next: Option<Gc<Mutex<Node>>>,
}

impl Drop for Node {
    fn drop(&mut self) {
        match self.next {
            Some(n) => {
                println!("The next node is {}", self.next.unwrap());
            },
            None => println!("No next node!"),
        }
        println!("Dropping {:?}", self.next);
    }
}

fn main() {
    let a = Gc::new(Mutex::new(Node {
        name: String::from("a"),
        next: None,
    }));

    let b = Gc::new(Mutex::new(Node {
        name: String::from("a"),
        next: None,
    }));

    a.lock().unwrap().next.insert(b);
}
\end{lstrustsmall}
    \caption{An example of a potentially unsound Rust program. Two
    \lstinline{Gc<Mutex<Node>>} objects (\lstinline{a} and \lstinline{b}) are
    created where \lstinline{a} contains a reference to \lstinline{b}. When
    these objects are garbage collected, \ourgc will schedule their finalisers
    to run in a non-specified order. If \lstinline{b} is finalised before
    \lstinline{a}, then \lstinline{a}'s drop method will access a dropped
    value.}
    \label{fig:unsound_finalisation_cycle}
\end{figure}

\subsubsection{Making unordered finalisation sound}
\label{sound_unordered_finalisation}

When \ourgc is compiled with unordered finalisation, it prevents drop methods
from dereferencing fields which point to other \lstinline{Gc} objects.
\autoref{fig:unsound_finalisation_cycle_error} shows the error message that is
displayed when the example in \autoref{fig:unsound_finalisation_cycle} is
compiled. It has identified that the user is trying to access field \lstinline{b},
which contains a \lstinline{Gc} type. I extend \ourgc's \emph{finaliser safety
analysis} (described in \autoref{fsa}) to detect this.

\begin{figure}[t!]
\begin{lstlisting}
error: `Mutex::new(Node {
               name: String::from("a"),
               next: None,
           })` cannot be safely finalized.
  --> src/main.rs:30:21
   |
15 |               Some(n) => {
   |                    -
   |                    |
   |                    caused by the expression here in `fn drop(&mut)` because
   |                    it uses another `Gc` type.
...
30 |       let b = Gc::new(Mutex::new(Node {
   |  _____________________^
31 | |         name: String::from("a"),
32 | |         next: None,
33 | |     }));
   | |______^ has a drop method which cannot be safely finalized.
   |
   = help: `Gc` finalizers are unordered, so this field may have already been dropped.
     It is not safe to dereference.

\end{lstlisting}
    \caption{The compile-time error message shown when attempting to compile the
    example in \autoref{fig:unsound_finalisation_cycle}. FSA identifies that
    another \lstinline{Gc} is being dereferenced inside a finaliser. This is
    unsound when \ourgc does not use ordered finalisation as it may have already
    been collected.}
    \label{fig:unsound_finalisation_cycle_error}
\end{figure}

This is implemented by first adding a negative implementation of
\lstinline{FinalizerSafe} to the \lstinline{Gc<T>} type:

\begin{lstrustsmall}
impl<T> !FinalizerSafe for Gc<T> {}
\end{lstrustsmall}

FSA then uses this to identify the specific field which was unsafely
dereferenced and will generates an error message different from those to do with
thread-safety.

However, as explained in \autoref{fsa}, FSA is not complete. It is possible that
a drop method could dereference a \lstinline{Gc} field in a way that FSA could
not detect e.g.~by doing so behind an opaque function call. In such cases
where the MIR for the entire drop method cannot be checked, FSA will err on the
side of caution, favouring soundness by refusing to compile the program.
\autoref{fsa_external} shows an example of this in practice.

\begin{figure}[t]
\begin{lstrustsmall}
use std::gc::Gc;

extern "C" {
    // A printer function written in "C" that is externally linked.
    // The compiler cannot see the contents of this function.
    fn node_printer(value: &Node);
}

struct Node {
    name: String,
    next: Option<Gc<Node>>,
}

impl Drop for Node {
    fn drop(&mut self) {
        unsafe {
            node_printer(self);
        }
    }
}

fn main() {
    let node = Gc::new(Node {
        name: String::from("a"),
        next: Some(Gc::new(Node {
            name: String::from("b"),
            next: None,
        })),
    });
}
\end{lstrustsmall}
    \caption{An example program which is forbidden in \ourgc. The drop method
    for \lstinline{Node} calls \lstinline{node_printer} on its value. This
    function is written in C and is externally linked, so the Rust compiler is
    unable to view its contents. Because of this, it cannot know whether or not
    \lstinline{node_printer} accesses the \lstinline{next} field. \ourgc thus
    conservatively rejects the program. If the user is certain that
    \lstinline{node_printer} does not access the \lstinline{next} field, they
    can inform \ourgc by implementing the \lstinline{FinalizerSafe} trait on
    \lstinline{Node}. }
    \label{fsa_external}
\end{figure}


\subsubsection{Unordered finalisation and the \bor}

\ourgc's \lstinline{Gc<T>} type provides an \lstinline{as_ref} function for
obtaining a reference to the underlying type (\lstinline{&T}). This allows the
\lstinline{Gc<T>} to be used in Rust code which does not care about how the
value is stored, but instead just expects a Rust reference to it
(\lstinline{&T}). Unfortunately, this creates a backdoor where a reference
to another \lstinline{Gc} object could be used in a finaliser. In such a case,
FSA can not tell that dereferencing a \lstinline{&T} accesses memory in
another \lstinline{Gc} object, because the ``\lstinline{Gc}'' part of the type
has been omitted. \autoref{borpot} shows a potential example of a raw reference
can leak into a finaliser.

\begin{figure}[t!]
\begin{lstrustsmall}
use std::gc::Gc;
use std::fmt::Debug;

struct Wrapper<T: Debug>(T);

impl<T: Debug> Drop for Wrapper<T> {
    fn drop(&mut self) {
        println!("Dropping {:?}", self.0);
    }
}

fn main() {
    let gc1 = Gc::new(123);
    let r1 = gc1.as_ref();

    // Pass a rust reference to `Wrapper`, hiding the fact that it points to
    // another GC object, which would be unsound to deref in the finaliser.
    let gc2 = Gc::new(Wrapper(r1));
}
\end{lstrustsmall}
    \caption{A reference to the \lstinline{Gc} object on line 13 is passed as a
    Rust reference to another \lstinline{Gc} (line 18). The drop method for
    \lstinline{Wrapper} dereferences this, which is unsound because if the
    finalisation order is unspecified.}
    \label{borpot}
\end{figure}

Fortunately, this problem is solved by the \bor (\autoref{sec:bor}), and the
example above would not compile with the error showing in \autoref{borwarn}

\begin{figure}[t!]
\begin{lstlisting}
error: `Wrapper(r1)` cannot be safely constructed.
  --> src/main.rs:18:23
   |
18 |     let gc2 = Gc::new(Wrapper(r1));
   |               --------^^^^^^^^^^^-
   |               |       |
   |               |       contains a reference (&) which may no longer be
   |               |       valid when it is finalized.
   |               `Gc::new` requires that a type is reference free.

error: could not compile `bin` due to previous error; 1 warning emitted
\end{lstlisting}
    \caption{Compiler error for \autoref{borpot} when run with \ourgc.}
    \label{borwarn}
\end{figure}


\section{Early finalisation}

\label{sec:early_finalisation}

A fundamental assumption in Rust's destructor semantics is that dropping a value
is the last thing to happen to it. The Rust compiler prevents using a value
after it has been dropped as this would cause unsoundness. For \lstinline{Gc}
values in \ourgc, the same must be true for finalisers. If a finaliser is able
to run before the mutator has finished using it, this would also be
unsound.

In \autoref{bg:early_finalisation}, I explain how finalisers can run earlier
than expected because compiler optimisations -- unaware of the presence of a GC
-- can cause GC objects to become unreachable earlier than expected. If this is
paired with an unfortunately timed GC cycle, the object's finaliser could run
while the object is still in use by the mutator. Rust and \ourgc is no
different: the Rust compiler is allowed to perform any optimisation that does
not change the observable behaviour of the program, and such optimisations are
not aware of the retro-fitted collector.

A finaliser which runs early can cause finalisation code to interleave
unexpectedly with the mutator~\citep{boehm03destructors}. But, even worse, in \ourgc early
finalisation can even lead to a memory safety violation. Consider the following
example, which shows how a finaliser which runs early could cause a
use-after-free:

\begin{lstrustsmall}
fn main() {
    let a = Gc::new(Box::new(123));
    let ref box_ptr = &*a;

    ...

    // Gc happens here causing
    // a's finaliser to drop the box.

    println!("{}", box_ptr); // Potential use-after-free!
}
\end{lstrustsmall}

Assuming that the compiler clobbers the reference the \lstinline{Gc} stored in
variable \lstinline{a}, this program can be represented as follows:

\begin{center}
\includegraphics[width=0.75\textwidth]{images/early_finalisation}
\end{center}

In this program, semantically, both \lstinline{a} and \lstinline{box_ptr} live
until the end of \lstinline{main}. However, the compiler may decide to reuse the
register holding the reference at \lstinline{a} any time after line 3 as it is
no longer used. An unfortunately timed GC cycle which happens immediately
afterwards would consider the \lstinline{Gc} object unreachable. Its finaliser
will then be run, freeing the Box. This would happen even though there is still
a reference (\lstinline{box_ptr}) to the inner \lstinline{Box} value. This
reference is now a dangling reference, and its use on line 10 would constitute a
use-after-free.

The possibility of early finalisation has led many VMs to specify that finalisers
can happen at any time -- even earlier than when an object becomes unreachable
(see \autoref{bg:early_finalisation}). One way of preventing early
finalisation in \ourgc would be to prevent \lstinline{Gc}
objects from owning non garbage collected objects, but this would render
\ourgc almost unusable. Fortunately, we can do better.

\subsection{Early finaliser prevention in \ourgc}

\label{early_finaliser_prevention}

To understand how early finaliser prevention works in \ourgc, lets revisit the
example from the previous section:

\begin{lstrustsmall}
fn main() {
    let a = Gc::new(Box::new(123));
    let ref box_ptr = &*a;

    ...

    // Gc happens here causing
    // a's finaliser to drop the box.

    println!("{}", box_ptr); // Potential use-after-free!
}
\end{lstrustsmall}

The problem is that the normal Rust compiler is not aware that a conservative GC
exists, and that line 8 is dependent on the reference at \lstinline{a} still
being reachable.

To fix this, \ourgc needs to ensure that the reference at
\lstinline{a} exists for the entire duration of \lstinline{main}. The basis of
a solution is to realise that inserting a \emph{compiler barrier}
-- which prevents reads and writes of specified variables from
being reordered before/after the barrier -- at the end of
\lstinline{main}, followed by an artificial read of \lstinline{a} keeps
references around sufficiently long.

In other words, \ourgc wants to
convert the example above to look roughly as follows:

\begin{lstrustsmall}
fn main() {
    let a = Gc::new(Box::new(123));
    ...

    COMPILER_BARRIER(a)
    *a;
}
\end{lstrustsmall}


\subsubsection{Using drop to insert Compiler barriers}

\ourgc takes advantage of two observations: Rust already inserts calls to \lstinline{drop} at
the same point in a function where we want to insert compiler barriers;
and we only need to insert barriers for variables of type \lstinline{Gc}.
However, since \lstinline{Gc} is a \lstinline{Copy} type, Rust prevents
us from adding a \lstinline{drop} method to \lstinline{Gc}.

Fortunately, since \ourgc already alters the Rust compiler, it is easy
to add a further modification. I thus modify the Rust
compiler to allow for simultaneous implementation of \lstinline{Copy} and
\lstinline{Drop} for \lstinline{Gc} types only, with the following drop
implementation:

\begin{lstrustsmall}
impl<T: ?Sized> Drop for Gc<T> {
    fn drop(&mut self) {
        unsafe {
            COMPILER_BARRIER(self)
        }
    }
}
\end{lstrustsmall}

The \lstinline{COMPILER_BARRIER(a)} includes inline assembly using Rust's
\lstinline{asm!} macro to create a read of the \lstinline{Gc}'s \lstinline{self}
reference after a compiler barrier. This is platform specific: for x86 it
translates to the following:

\begin{lstlisting}
asm("":::"memory")
\end{lstlisting}

Although the compiler barrier does not contain platform instructions, its
format is still platform dependent: other platforms such as AArch64 may
require a slightly different \lstinline{asm} statement.

However, the compiler barrier by definition prevents the compiler from
performing some of its normal optimisations --- it is an expensive solution to a
rare problem. In our performance analysis, this had roughly a 2-3\% slowdown. In
\autoref{optimising_early_finalisers} I describe how I optimise this approach,
removing barriers where it can be statically determined that they are
unnecessary.

\section{Optimising finalisers}

\label{opt_fin}

\subsection{Finaliser elision}
\label{sec:finaliser_elision}

For performance reasons, many GC'd languages discourage programmers from using finalisers.
In Rust, since \lstinline{drop} is ubiquitous, mapping drop methods to
finalisers therefore have a high performance overhead.

To claw back this performance hit, I implement a new optimisation called
\textit{finaliser elision}. This optimisation is based on the observation that
sometimes only the top-most object in a \lstinline{Gc} graph needs to have a
finaliser. Consider the following example:

\begin{lstrustsmall}
let a = Box::new(123);
let b = Gc::new(a);
\end{lstrustsmall}

Here \lstinline{a} refers to a boxed integer on the heap which is not managed by
the collector. It is placed inside a \lstinline{Gc}, which has the following
representation in memory:

\begin{center}
\includegraphics[width=0.75\textwidth]{images/finaliser_elision_1}
\end{center}

Before the \lstinline{Gc} referenced by \lstinline{b} is collected, its
finaliser is called, which calls drop on the non-GC'd box:

\begin{center}
\includegraphics[width=0.75\textwidth]{images/finaliser_elision_2}
\end{center}

This is inefficient because the collector would have later reclaimed the
\lstinline{Box}, since all references to it would be lost. In other words,
neither \lstinline{a}'s finaliser nor \lstinline{b}'s drop method need
to be called for the memory to be reclaimed.

Since sweeping is cheaper than finalisation, finaliser elision aims to identify
other cases where finalisation can be avoided. It is a powerful optimisation,
but not applicable everywhere as this example shows:

\begin{lstrustsmall}
struct HasDrop(u64);

impl Drop for HasDrop {
    fn drop(&mut self) {
        println!("Dropping HasDrop");
    }
}

let a = Gc::new(HasDrop(123));
\end{lstrustsmall}

Here \lstinline{a} refers to a \lstinline{Gc} containing an integer, which has
the following representation in memory:

\begin{center}
\includegraphics[width=0.5\textwidth]{images/finaliser_elision_3}
\end{center}

We are not able to elide the finaliser for this \lstinline{Gc} because, unlike
the previous example, the drop method does more than just drop another heap
object in this case printing to stdout.

\subsubsection{When can a finaliser be elided?}

The foundation of finaliser elision lies with the
\lstinline{FinalizerOptional} trait, which is used to determine if a type needs
finalising. If a type \lstinline{T} implements \lstinline{Drop}, but also
implements \lstinline{FinalizerOptional}, then \lstinline{Gc<T>} won't be
finalised. For example we can adjust our earlier example as follows:

\begin{lstrustsmall}
struct HasDrop;

impl Drop for HasDrop {
    fn drop(&mut self) {
        println!("Dropping HasDrop");
    }
}

unsafe impl FinalizerOptional for HasDrop {}

let a = Gc::new(HasDrop(123));
\end{lstrustsmall}

This informs \ourgc that \lstinline{HasDrop} does not need a finaliser
when placed inside a \lstinline{Gc}, even though \lstinline{HasDrop}
implements the \lstinline{Drop} trait. When a \lstinline{Gc} is constructed
with a value of this type, it no longer has a finaliser:

\begin{center}
\includegraphics[width=0.5\textwidth]{images/finaliser_elision_4}
\end{center}

\lstinline{FinalizerOptional} is particularly powerful when used
with container types. For example, consider the standard Rust
\lstinline{Box<T>} type. Its heap memory can be automatically reclaimed by the
allocator, but depending on the type \lstinline{T}, we may
need to call a finaliser. Thus we cannot simply always remove a \lstinline{Box}'s finaliser.
Fortunately we can easily tell Rust's type system that we want to make
\lstinline{Box<T>} be \lstinline{FinalizerOptional} if \lstinline{T}
is also \lstinline{FinalizerOptional} with:

\begin{lstrustsmall}
unsafe impl<T> FinalizerOptional for Box<T> {}
\end{lstrustsmall}

This tells \ourgc that it's safe to elide the finaliser for \lstinline{Box<T>}
if \lstinline{T} does not need finalising. When \ourgc discovers a type which
implements \lstinline{FinalizerOptional}, it will treat it as if it does not
implement \lstinline{Drop}, but continue on checking all its component types.

\ourgc implements \lstinline{FinalizerOptional} on the following heap allocating
standard library types: \lstinline{Box<T>}, \lstinline{Vec<T>},
\lstinline{RawVec<T>}, \lstinline{HashMap<T>}. This is enough to elide a
significant amount of finalisers without any extra effort required by the user
(see \autoref{fincost}). If the user defines their own heap
allocating types which use drop to deallocate, they can implement
\lstinline{FinalizerOptional} on it so that it can also benefit from finaliser
elision.


\subsubsection{Finaliser elision algorithm}

The exact algorithm for finaliser elision detection is defined as follows:

\begin{algorithm}
\begin{algorithmic}
\Function{NeedsFinaliser}{$type$}
    \If {\Call{Impls}{$type$, $Drop$} \AND \NOT \Call{Impls}{$type$, $FinalizerOptional$}}
        \State \Return{true}
    \EndIf
\ForEach {$component \in type$}
    \If{\Call{NeedsFinaliser}{$component$}}
        \State \Return{true}
    \EndIf
\EndFor
\State \Return{false}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{figure}
    \centering
    \subfloat[\centering \lstinline{Gc<Box<u64>>}]{\includegraphics[width=0.3\textwidth]{images/finaliser_elision_5}}
    \hfill\hspace{0.2\textwidth}
    \subfloat[\centering \lstinline{Gc<Box<Box<u64>>>}]{\includegraphics[width=0.3\textwidth]{images/finaliser_elision_7}} \\
    \vspace{30px}
    \subfloat[\centering \lstinline{Gc<Box<HasDrop>>}]{\includegraphics[width=0.4\textwidth]{images/finaliser_elision_6}}
    \hfill
    \subfloat[\centering \lstinline{Gc<Box<Box<HasDrop>>>}]{\includegraphics[width=0.4\textwidth]{images/finaliser_elision_8}}
    \caption{The memory layout of various \lstinline{Gc} types.
    \textbf{(a)} and \textbf{(b)} do not need a finaliser because the \lstinline{Box} implements
    \lstinline{FinalizerOptional} and \lstinline{u64} does not need finalising.
    \textbf{(c)} and \textbf{(d)} do need a finaliser because in
    each example, a \lstinline{Box} contains a \lstinline{HasDrop}, which needs
    finalising because it has a drop method. Notice that for \textbf{(c)} and
    \textbf{(d)}, the finaliser calls the drop methods for each component type --
    even the boxes which are marked \lstinline{FinalizerOptional}. This is
    because finaliser elision does not do partial elision of finalisers.}
\label{fig:finaliser_elision}
\end{figure}

\autoref{fig:finaliser_elision} shows various examples of when finaliser
elision can remove a finaliser, and when it cannot. What's important to note
here is that if a type \lstinline{T} is marked \lstinline{FinalizerOptional},
but has fields which need finalising, then \lstinline{T} will still be
finalised.

The algorithm for finaliser elision is implemented as a compiler intrinsic,
\lstinline{needs_finalizer<T>()} which returns true if \lstinline{Gc<T>} needs a
finaliser. This intrinsic is then called during the construction of new
\lstinline{Gc} objects (inside \lstinline{Gc::new}):

\begin{lstrustsmall}
impl Gc<T> {
    pub fn new(value: T) -> Self {
        ...
        if needs_finalizer::<T>() {
            Gc<T>::new_with_finalizer(value)
        } else {
            Gc<T>::new_without_finalizer(value)
        }
        ...
    }
}
\end{lstrustsmall}

\lstinline{needs_finalizer<T>} is marked as a special type of Rust function,
called a \emph{const} function, which means that it is evaluated at compile
time, this means that the conditional on line 4 has no runtime cost.

\subsubsection{Sweeping objects with elided finalisers}

Once an object's finaliser has been elided, \ourgc needs to be responsible for
managing its memory. This is achieved with a modification to the global
allocator in Rust. Every heap allocation in Rust programs compiled with \ourgc
is allocated using BDWGC's \lstinline{GC_malloc} function --- even those which
are not part of a \lstinline{Gc}. For values which are allocated using Rust's RAII
approach, they are manually deallocated with calls to \lstinline{GC_free}. For
non-GC'd heap allocations, this is semantically equivalent to a regular
\lstinline{malloc} call: there will always be a reference preventing them from
being collected, and RAII ensures that the corresponding \lstinline{GC_free} is
called the moment they go out of scope. However, when such allocations are owned
by a \lstinline{Gc}, they will be freed by the collector along with the
\lstinline{Gc}.

\subsection{Optimising early finaliser prevention}
\label{optimising_early_finalisers}

Early finalisation prevention (\autoref{early_finaliser_prevention})
overapproximates the places where early finalisation can happen, which can have
a significant impact on performance. Fortunately, the finaliser elision
optimisation in \autoref{finaliser_elision} shows that many finalisers never
need to be called, at which point we also no longer have to worry early
finalisation! Where this is the case, we are able to remove the drop method for
the \lstinline{Gc<T>} pointers which contain compiler barriers during
compilation.

All \lstinline{Gc} values have drop methods with barriers by default. During
compilation, barriers which we can prove are unnecessary are removed. This is
done once the Rust compiler has generated its mid-level IR (MIR). Like finaliser
safety analysis (\autoref{fsa}), we perform an in-order traversal on the control
flow graph represented by the MIR for each function with the following
algorithm:

% UnneededBarrierRemoval(prog):
%     for mir_body in prog:
%         for basic_blocks in mir_body:
%             for block in basic_blocks:
%                 if block.terminator == DROP_CALL:
%                     arg = ..;
%                     arg_ty = ..;
%                     if IS_TEMPORARY(arg) || not NEEDS_FINALIZER(arg_ty):
%                         RemoveDrop(block)

% RemoveDrop(block):
%     drop_mir = GetDestMirBody(block.terminator)
%     last_block = LastBlock(drop_mir)
%     new_mir = GetDestMIRBody(last_block.terminator)

%     #  patch old block with new block
%     new_term = MakeCallTerminator(new_mir)
%     block.terminator = new_term


\begin{algorithm}
\begin{algorithmic}
\Function{BarrierRemoval}{$callsite$}
    \ForEach {$mir\_body \in prog$}
        \ForEach {$basic\_blocks \in mir\_body$}
            \ForEach {$block \in basic\_blocks$}
                \If {\Call{CallsDrop}{$block.terminator$}}
                    \State $arg \gets$ \Call{GetFirstArg}{$block.terminator$}
                    \State $arg\_ty \gets$ \Call{GetType}{$arg$}
                    \If {\Call{IsGC}{$arg\_ty$}}
                        \If {\NOT \Call{NeedsFinaliser}{$arg\_ty$}}
                            \State \Call{RemoveDrop}{$projection$}
                        \EndIf
                    \EndIf
                \EndIf
            \EndFor
        \EndFor
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

This iterates over all drop methods in the entire program, identifying those
which belong to a \lstinline{Gc<T>}. If found, the drop call is removed if the
Gc reference points to an object which does not need finalising. The drop method
is removed by patching the terminator of the block which calls drop with the
terminator at the end of the drop body:

\begin{algorithm}
\begin{algorithmic}
\Function{RemoveDrop}{$block$}
    \State $drop\_mir \gets$ \Call{GetDropMirBody}{$block.terminator$}
    \State $last\_block \gets$ \Call{GetLastBlock}{$drop\_mir$}
    \State $block.terminator \gets last\_block.terminator$
\EndFunction
\end{algorithmic}
\end{algorithm}

After this pass, we call the Rust compiler's existing \emph{simplify mir} pass,
which tidies up the control flow graph by removing the empty blocks which were
created as a result of drop removal.

\section{\ourgc performance evaluation}
\label{ch:perf}

This section examines the performance characteristics of \ourgc. To do this, I
present five performance studies:

\begin{enumerate}
    \item An evaluation of the wall-clock time and memory usage of \ourgc as a
        candidate for use in a single-threaded VM. This compares \ourgc with
        Rust's single-threaded reference counting library on the SOM benchmark
        suite~\citep{marr16cross} when used to manage objects in the \somrs VM (\autoref{somstudy}).
    \item An evaluation of the wall-clock time and memory usage of \ourgc as a
        candidate for use in a multi-threaded VM. This compares \ourgc with
        Rust's atomic reference counting library using the \wlambda VM
        (\autoref{wlambdastudy}).
    \item An evaluation of the effectiveness of the finaliser elision
        optimisation. This uses two configurations of the \yksom VM -- with and
        without finaliser elision -- to measure its performance impact using the
        SOM benchmark suite (\autoref{finoverhead}).
    \item An evaluation of the slowdown introduced by the early finaliser
        prevention mechanism in \ourgc (\autoref{fincost}).
    \item A comparison between \ourgc and other GC implementations in Rust on
        the Binary Trees benchmark (\autoref{othergcs}).
\end{enumerate}

\subsection{Generic benchmarking methodology}

The four performance studies share a common benchmarking methodology. All
experiments were run on a Xeon E3-1240v6 3.7GHz with 4 CPU cores. Each core has
an L1 and L2 cache size of 256KB and 1MB respectively. The 8MB L3 cache is shared
between cores. The benchmarking machine has 32GB of RAM and runs Debian 8. I
disabled turbo boost and hyper-threading in the BIOS and set Linux's CPU
governor to \emph{performance} mode. I observed the \texttt{dmesg} after each
experiment and did not observe any oddities such as the machine overheating.

For \ourgc I use an initial heapsize of 64KB and the heap is always expanded
when the collector encounters insufficient space for an allocation. A collection
is triggered whenever more than \lstinline{heap_size / 6} bytes of allocation
have taken place. This is the default behaviour in the \boehm.

\subsection{\ourgc for language implementation}

One of \ourgc's main goals is to be usable for implementing other languages VMs.
In order to evaluate the effectiveness of this, this section aims to answer the
following two questions:

\begin{enumerate}
    \item How much slower is a language VM written in Rust when using \ourgc
        compared to Rust's reference counting library?
    \item How much memory does a language VM written in Rust when using \ourgc
        use when compared to Rust's reference counting library?
\end{enumerate}

To answer these questions I conduct two studies, where I retrofit \ourgc to two
separate language VMs:

\begin{description}
    \item[\somrs] A VM written in Rust for the SOM language (a cut down
        Smalltalk language) which uses Rust's single threaded reference counting
        library (\lstinline{Rc}) to manage SOM objects.
    \item[WLambda] A VM written in Rust for the WLambda language: a
        multi-threaded JavaScript-like scripting language which uses Rust's
        atomic reference counting library (\lstinline{Arc}) to manage SOM
        objects.
\end{description}

This allows a performance comparison between VMs which use \ourgc and both forms
of reference counting in otherwise identical language implementations. The
following subsections outline the methodology and results of each experiment.

\subsection{First study: the \somrs VM}
\label{somstudy}


\subsubsection{Methodology}

The first study is a performance and memory comparison using \somrs~\citep{somrs}
-- a Rust implementation for the SOM language (a cut down version of Smalltalk).
\somrs does not support Just-in-Time (JIT) compilation and is a medium sized
interpreter with roughly 10KLoC. I build two configurations of \somrs as
follows:

\begin{description}
    \item[\somrsrc] the standard implementation, where SOM objects are managed using \lstinline{Rc} (Rust's
        single-threaded reference counting mechanism).
    \item[\somrsgc] where \somrs is modified so that SOM objects are managed
        using \ourgc. This required changing 62 lines of code, most of which was
        replacing instances of \lstinline{Rc} with \ourgc's \lstinline{Gc}. Both
        configurations are otherwise identical.
\end{description}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{graphs/som_bar.pdf}
    \caption{Results from the \perf experiment, where the SOM benchmark suite is
    run with both configurations: \somrsrc, and \somrsgc. The vertical bars
    represent the wall-clock time of \somrsgc, normalised to the wall-clock time
    of \somrsrc. The dashed grey line shows the geometric mean of the slowdown
    of \somrsgc across all benchmarks (1.12$\times$). Each benchmark is run for
    30 process executions, where the error bars represent 99\% confidence
    intervals.}
\label{graph:som_bar}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{graphs/som_bar_new.pdf}
    \caption{Results from the \perf experiment, where the SOM benchmark suite is
    run with both configurations: \somrsrc, and \somrsgc. The vertical bars
    represent the wall-clock time of \somrsgc, normalised to the wall-clock time
    of \somrsrc.}
\label{graph:som_bar_new}
\end{figure}

I use the SOM language's existing suite of synthetic benchmarks, adapted from
the \emph{are-we-fast-yet} benchmark suite~\citep{marr16cross}. The benchmarks are
written using idiomatic SOM code.

The study is divided into two sub-experiments which measure performance and
memory separately:

\begin{description}
    \item[\perf] In the performance experiment, I compare the performance
        between \somrsrc, and \somrsgc using the Rebench benchmarking
        suite~\citep{rebench18marr}. Each SOM benchmark is run for 30
        \emph{process executions}, where each VM is closed down and started
        again. Rebench randomises the order in which each VM implementation is
        run after each process execution and records wall-clock times using
        Python's \texttt{time()} function.

    \item[\mem] In the memory experiment, memory usage from both
        configurations is measured by running each benchmark individually while
        sampling the process's \emph{resident set size} every 10 milliseconds as
        this was the smallest value where there were noticable differences
        between samples. From this, I present the peak and average real (~i.e.
        physical, not virtual) memory usage for each benchmark as well as
        plotting the usage over time.
\end{description}

\subsubsection{Results}

\makeatletter
\newcommand*\ExpandableInputSomrsMemory[1]{\@@input#1 }
\makeatother
 \begin{table*}[t!]
     \centering
     \begin{tabular}{lllll}
 \toprule
         \multirow{2}{*}{Benchmark} & \multicolumn{2}{c}{Peak memory usage (MB)} & \multicolumn{2}{c}{Average memory usage (MB)} \\
\cmidrule(l){2-3} \cmidrule(l){4-5}
         & Reference Counting & \ourgc & Reference Counting & \ourgc \\
 \midrule
\ExpandableInputSomrsMemory{./somrs_memory_table.tex}
 \bottomrule
     \end{tabular}
 \vspace{6pt}
         \caption{Peak and average memory usage (in megabytes) of each SOM
         benchmark for both som-rs configurations.}
         \label{somrs_memory_table}
\end{table*}

\autoref{graph:som_bar} shows the performance results for the \perf experiment.
On average, \somrsgc performs 12\% worse, when taking the geometric mean across
all benchmarks. The worst performing benchmarks are \emph{Loop},
\emph{Recurse}, and \emph{Richards} which each have a slowdown of
1.24$\times$. One possible explanation for this is that for these benchmarks in
particular, many of the allocated objects have a lifetime which extends for the
duration of the benchmark. This means that frequent GC pauses by \ourgc free
little memory but end up increasing the total wall-clock time of the benchmark.
For example, during the Richards benchmark, a GC collection cycle was scheduled
an average of 24,800 times for each iteration.

\autoref{somrs_memory_table} shows the memory usage results from the \mem
experiment. For most benchmarks, \somrsgc consumes slightly more memory. Though
\ourgc will use additional memory for its internal data structures for
bookkeeping and during collection, this does not sufficiently account for the
additional memory used by \somrsgc. It is more likely that objects in \somrsgc
are being retained for longer than in \somrsrc. It is not clear why this is the
case, but one possible reason is that \somrs uses a third party hash map
implementation internally for locals and method storage, and once references are
removed, they may not be properly zeroed, causing stale pointers to keep objects
alive. \autoref{fig:somrs_gc_high} highlights two cases in particular,
\emph{PageRank} and \emph{GraphSearch}, where memory usage in \somrsgc is
unusually high. The graphs showing memory usage over time for all benchmarks can
be found in \autoref{allgraphs}.

\begin{figure}[t]
    \centering
    \subfloat{\includegraphics[width=0.5\textwidth]{graphs/pagerank_memory.pdf}}
    \hfill
    \subfloat{\includegraphics[width=0.5\textwidth]{graphs/graphsearch_memory.pdf}}
     \caption{Real memory usage of the PageRank and GraphSearch benchmarks over
     time. In both of these benchmarks, \somrsgc uses significantly more memory.
     While this suggests there is a memory leak somewhere in \somrsgc, it is not
     yet clear why. One likely explanation which I have come across before
     during development is that during allocation heavy tight loops, stale
     references to heap objects may still exist in not-yet-overwritten stack
     slots which are then traced conservatively during marking.}
     \label{fig:somrs_gc_high}
\end{figure}

\begin{figure}[t]
    \centering
    \subfloat{\includegraphics[width=0.5\textwidth]{graphs/deltablue_memory.pdf}}
    \hfill
    \subfloat{\includegraphics[width=0.5\textwidth]{graphs/jsonsmall_memory.pdf}}
     \caption{Real memory usage of the DeltaBlue and
     JsonSmall benchmarks over time. Both of these benchmarks allocate objects
     containing reference cycles, which \somrsrc is unable to free due to its
     use of reference counting. The resulting leak causes \somrsrc's real memory
     usage to grow continously over time. \somrsgc does not have this problem
     because \ourgc is a tracing garbage collector.}
         \label{fig:somrs_rc_leak}
\end{figure}

However, \ourgc's advantage of being able to cope with objects with cyclic
references is shown clearly in \autoref{fig:somrs_rc_leak}. \emph{DeltaBlue} and
\emph{JsonSmall} when run with \somrsrc leak memory because these benchmarks
contain objects with cycles. For these benchmarks, \somrsrc leaks memory so
severely that they would cause cause out-of-memory errors if ran for just a few
minutes, while \somrsgc maintains a steady heap profile over time.

\clearpage

\subsection{Second study: the \wlambda VM}
\label{wlambdastudy}

\subsubsection{Methodology}

The second study is a performance and memory comparison using
\wlambda~\citep{wlambda} -- a Rust VM for an embeddable perl-like scripting language.
\wlambda is a medium-sized VM with approximately 30KLoC. It supports prototype
based object orientation, and most \wlambda objects are built upon vector and
map data structures. Unlike \somrs, \wlambda is multi-threaded, so it allows for
a comparison of a VM using \ourgc and atomic reference counting. I build two
configurations of \wlambda as follows:

\begin{description}
    \item[\wlambdaarc] the standard implementation, where VM objects are managed
        using \lstinline{Arc} (Rust's atomic reference counting mechanism).
    \item[\wlambdaalloy] where \wlambda is modified so that VM objects are managed
        using \ourgc. This required changing roughly 700 lines of code, most of which was
        replacing instances of \lstinline{Rc} with \ourgc's \lstinline{Gc}.
        \wlambda makes use of Rust's trait-based dynamic dispatch, so each trait
        was manually audited to see whether it could be marked
        \lstinline{FinalizerSafe} to satisfy FSA (\autoref{fsa}), or
        \lstinline{FinalizerOptional} to use finaliser elision
        (\autoref{finaliser_elision}). Both
        configurations are otherwise identical.
\end{description}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{graphs/wlambda_bar.pdf}
    \caption{Results from the \perf experiment, where the \wlambda benchmarks
    are run with both configurations: \wlambdaarc, and \wlambdaalloy. The
    vertical bars
    represent the wall-clock time of \wlambdaalloy, normalised to the wall-clock time
    of \wlambdaarc. The dashed grey line shows the geometric mean of the slowdown
    of \wlambdaalloy across all benchmarks (1.14$\times$). Each benchmark is run for
    30 process executions, where the error bars represent 99\% confidence
    intervals.}
\label{graph:wlambda_bar}
\end{figure}


\wlambda comes with four benchmarks which I use to test each configuration:
\emph{Pattern Matching}, which stresses comparisons performed on objects;
\emph{Fibonacci}, which stresses function call overhead and recursion; \emph{Vec
Iter}, which creates and manipulates large lists and maps; and \emph{Box}, which
allocates many objects and stresses read and write operations.

Like the first \somrs study, this study is also divided into two sub-experiments
to measure performance and memory separately:

\begin{description}
    \item[\perf] In the performance experiment, I compare the performance
        between \wlambdaarc, and \wlambdaalloy using the \emph{multitime} tool:
        a Unix \emph{time} utility extension which allows commands to be run
        multiple times for comparison~\citep{multitime}. I run benchmarks for
        each configuration for 30 process executions. multitime randomises the
        order in which each benchmark and VM configuration is run, and records
        wall-clock time. I report the mean wall-clock time of each benchmark
        with 99\% confidence intervals.

    \item[\mem] Similar to the \yksom experiment, the memory usage is of both
        configurations is measured by running each benchmark individually while
        sampling the process's \emph{resident set size} every 10 milliseconds as
        this was the smallest value where there were noticable differences
        between samples.
\end{description}



\subsubsection{Results}

\autoref{graph:wlambda_bar} shows the performance results for the \perf
experiment. The range in performance on this suite is large, with \wlambdaalloy
performing 18\% better on \emph{Pattern Matching} but 45\% slower on \emph{Box}.
On average, \wlambdaalloy performs 14\% worse, when taking the geometric mean
across all benchmarks.

\autoref{table:wlambda_memory} shows the memory usage results from the
\mem experiment while \autoref{wlambda_over_time} shows the memory usage of each
benchmark over time. In all benchmarks, \wlambdaalloy uses more memory. This is
particularly apparent in the \emph{Box} benchmark, where on average
\wlambdaalloy requires 4MB of additional memory suggesting that \wlambdaalloy
has a prominent memory leak for this benchmark. In the \perf experiment, this is
also the worst performing benchmark. \emph{Box} allocates a total of 450,027,871
objects which results in 30\% of the runtime spent in marking when profiling the
benchmark in perf.

\makeatletter
\newcommand*\ExpandableInputWLambdaMemory[1]{\@@input#1 }
\makeatother
 \begin{table*}[t!]
     \begin{center}
     \begin{tabular}{lllll}
 \toprule
         \multirow{2}{*}{Benchmark} & \multicolumn{2}{c}{Peak memory usage (MB)} & \multicolumn{2}{c}{Average memory usage (MB)} \\
\cmidrule(l){2-3} \cmidrule(l){4-5}
         & Atomic RC & \ourgc & Atomic RC & \ourgc \\
 \midrule
\ExpandableInputWLambdaMemory{./wlambda_memory_table.tex}
 \bottomrule
     \end{tabular}
 \vspace{6pt}
         \caption{Peak and average memory usage (in megabytes) of each WLambda
         benchmark for both configurations.}
         \label{table:wlambda_memory}
     \end{center}
\end{table*}


\begin{figure}[t!]
    \centering
    \subfloat{\includegraphics[width=0.5\textwidth]{graphs/jmptbl_memory.pdf}}
    \hfill
    \subfloat{\includegraphics[width=0.5\textwidth]{graphs/call_performance_memory.pdf}}
    \vspace{30px}
    \subfloat{\includegraphics[width=0.5\textwidth]{graphs/nvec_memory.pdf}}
    \hfill
    \subfloat{\includegraphics[width=0.5\textwidth]{graphs/box_vs_rs_memory.pdf}}
    \caption{Real memory usage of the \wlambda benchmarks over time.}
     \label{wlambda_over_time}
\end{figure}

\subsection{Measuring finaliser overhead}
\label{finoverhead}

\subsubsection{Methodology}
In my third study, I evaluate the effectiveness of \ourgc's finaliser elision
with a straightforward comparison between \ourgc with and without finaliser
elision. I use \yksom: a another SOM interpreter written in Rust which uses
\ourgc to manage SOM objects. \yksom is a lightweight interpreter with
approximately 5700KLoC. It is suitable for measuring the impact of finaliser
overhead because it makes frequent use of \lstinline{drop} methods inside the
VM. I build two configurations of \yksom as follows:

\begin{description}
    \item[\yksomelision] The standard \ourgc build which enables finaliser
        elision optimisation (\autoref{finaliser_elision}).
    \item[\yksomnaive] An otherwise identical build but with \ourgc's finaliser
        elision optimisation disabled.
\end{description}

I use the SOM benchmark suite using Rebench benchmarking suite using two
versions of \yksom for each \ourgc variant. I run each SOM
benchmark for 30 \emph{process executions}, where each instance of yksom is
closed down and started again. Rebench records wall-clock times using Python's
\texttt{time()} function. I also run each SOM benchmark and record the number of
finalisers that were executed for each \ourgc variant.

\subsubsection{Results}

\autoref{table:finaliser_elision} shows the performance results for this study.
Finaliser elision is able to remove over 99\% of finalisers on every benchmark,
leading to a 14.87\% improvement in runtime speed when taking the geometric mean
across all benchmarks. Such an improvement is clear in part due to \yksom's
internal VM object representation: \yksom objects use Strings and HashMaps
internally, which are allocated on the Rust heap and thus, without finaliser
elision, require tens of thousands of calls to drop in order to deallocate.

However, despite significant improvements in wall-clock time on nearly every
benchmark, \emph{FieldLoop} regresses by 10.59\%. The reason for this became
clear once I recorded the peak memory usage of \yksomnaive (3.25MB) and
\yksomelision (4.31MB). The finalisers for 606,385 objects are elided,
meaning that those objects become the responsibility of \ourgc to GC. Therefore,
based on the higher memory usage, the most likely reason for the resulting
slowdown is that those objects are being erroneously kept alive. After analysing
the program in perf, almost all the excess runtime is spent in marking.

\makeatletter
\newcommand*\ExpandableInputFinalisers[1]{\@@input#1 }
\makeatother
\begin{landscape}
 \begin{table*}[t]
     \centering
     \begin{tabular}{lllllrr}
 \toprule
    \multirow{2}{*}{Benchmark} & \multicolumn{2}{c}{\yksomnaive} & \multicolumn{4}{c}{\yksomelision} \\
\cmidrule(l){2-3} \cmidrule(l){4-7}
         & Runtime (ms) & Finaliser count & Runtime (ms) & Finaliser
         count & Runtime slowdown & Finalisers elided \\
 \midrule
\ExpandableInputFinalisers{./table_finalizers}
 \bottomrule
     \end{tabular}
 \vspace{6pt}
     \caption{Results from my finaliser elision experiment, where I evaluate the
     performance of yksom using two configurations: naive finalisation, and
     \ourgc's finaliser elision optimisation. Each configuration is compared
     using a subset of benchmarks on the Rebench benchmarking suite for 30
     process executions, where I report 99\% confidence intervals. I also count
     the number of finalisers which were run for each benchmark for a single
     process execution.}
\label{table:finaliser_elision}
\end{table*}
\end{landscape}


\subsection{Early finaliser prevention}
\label{fincost}

\makeatletter
\newcommand*\ExpandableInputBarriers[1]{\@@input#1 }
\makeatother
 \begin{table*}[t!]
     \centering
\scalebox{0.8}{
     \begin{tabular}{llllll}
 \toprule
    \multirow{2}{*}{Benchmark} & \multicolumn{1}{c}{\yksomnobarriers} & \multicolumn{2}{c}{\yksomallbarriers} & \multicolumn{1}{c}{\yksomoptbarriers} \\
\cmidrule(l){2-2} \cmidrule(l){3-4} \cmidrule(l){5-6}
         & Runtime (ms) & Runtime (ms) & Slowdown & Runtime (ms) & Slowdown \\
 \midrule
\ExpandableInputBarriers{./table_barriers.tex}
 \bottomrule
     \end{tabular}
     }
 \vspace{6pt}
     \caption{Results from my early finaliser prevention experiment, where I
     evaluate the performance of yksom using three configurations:
     \yksomnobarriers, where there are no compiler barriers (which is unsound);
     \yksomallbarriers, where every single \lstinline{Gc} reference has a
     corresponding barrier; and \yksomoptbarriers, where barriers can be
     optimised away where \ourgc is certain they are unnecessary. Each
     configuration is compared using a subset of benchmarks on the Rebench
     benchmarking suite for 30 process executions, where I report 99\%
     confidence intervals.}
     \label{table:barriers}
\end{table*}

\subsubsection{Methodology}

My fourth study aims to understand the performance impact of the early
finaliser prevention technique \ourgc uses to ensure finalisers are sound.
I aim to answer two questions:

\begin{enumerate}
  \item What is
the impact of the compiler barriers that \ourgc uses to prevent early
finalisation?
  \item How effective is the optimisation that alloy uses to
remove barriers where it can guarantee they are unnecessary?
\end{enumerate}

As with the previous study in \autoref{finoverhead}, I use \yksom to perform
this evaluation. I build three configurations of \yksom as follows:

\begin{description}
    \item[\yksomnobarriers] This compiles \yksom with a version of \ourgc with
        no compiler barriers to prevent possible early finalisation. This is
        unsound.
    \item[\yksomallbarriers] This compiles \yksom with a version of \ourgc which
        inserts a compiler barrier for every single \lstinline{Gc} reference.
    \item[\yksomoptbarriers] This compiles \yksom with a version of \ourgc which
        inserts a compiler barrier \lstinline{Gc} reference which cannot be
        optimised away with the approach described in
        \autoref{optimising_early_finalisers}.
\end{description}

I use the SOM benchmark suite using Rebench benchmarking suite using two
versions of \yksom for each \ourgc variant. I run each SOM
benchmark for 30 \emph{process executions}, where each instance of yksom is
closed down and started again. Rebench records wall-clock times using Python's
\texttt{time()} function. I also run each SOM benchmark and record the number of
finalisers that were executed for each \ourgc variant.

\subsubsection{Results}

\autoref{table:barriers} shows the performance results for this study.
As one would expect, inserting barriers for every GC reference causes a slowdown
on every benchmark. However, perhaps surprsingly, \yksomoptbarriers is faster
than \yksomnobarriers on the following benchmarks: Bounce, Fibonacci,
IntegerLoop, List, Loop, PageRank, Storage, Sum, and TreeSort. While I am not
certain why this is the case, I believe the most likely reason for this is that
sometimes, barriers can be inserted in such a way that they keep GC objects
alive for the duration of the shorter running benchmarks, thus preventing
finalisation for those objects entirely. Furthermore, when I disable the
scheduling of finalisers entirely for the \yksomoptbarriers configuration, each
previously of these benchmarks regressed from between 0.83\% to 3.32\%

\subsection{Comparison between other GCs}
\label{othergcs}

My fifth and final experiment aims to understand the performance costs of
\ourgc against other garbage collected approaches in Rust.

The overall question I would like this experiment to answer is: how fast is
\ourgc when compared with the other garbage collected options available in Rust?
While I have been able to provide a more detailed assessment of \ourgc's
performance when used to implement other languages, I am only able to provide
a limited comparison of \ourgc's performance relative to other collectors:
converting benchmarks to Luster's unusual approach was prohibitively
difficult; and Bronze crashed with many benchmarks. In addition, I had
difficultly finding suitable Rust programs which were practical enough to modify
to use the various tracing GC implementations while also performing enough heap
allocations to be useful. Fortunately, and despite these restrictions, this
experiment is still able to provide valuable insights.

In this experiment, I compare the performance of \ourgc against three different
approaches to managing memory: using a indexed-arena,
\rustgc (\autoref{alloy_related}), and Rust's standard reference counting library. I
run each configuration on the Binary Trees benchmark from the Computer Language
Benchmark Game for 30 process executions. The wall-clock times are
recorded before and after each process execution using the multitime tool.

\makeatletter
\newcommand*\ExpandableInputBinTrees[1]{\@@input#1 }
\makeatother
 \begin{table*}[t]
     \begin{center}
     \begin{tabular}{llll}
 \toprule
\ExpandableInputBinTrees{./table_binary_trees.tex}
 \bottomrule
     \end{tabular}
 \vspace{6pt}
     \caption{Results from my experiment comparing \ourgc against three other
         garbage collection configurations on the Binary Trees benchmark for 30
         process executions, where I report 99\% confidence intervals. This
         clearly shows that except for the index-arena (which deallocates all
         its memory at once) \ourgc is the fastest configuration.}
         \label{table:binary_trees}
     \end{center}
\end{table*}

\subsection{Results}

\autoref{table:binary_trees} shows the results for this experiment. The results
shows that for Binary Trees (an allocation heavy benchmark)
\textsc{Typed-arena} was the fastest as it never performs any deallocation
during the benchmark run, it simply deallocates all memory at the end.

\rustgc performs poorly for two reasons. First, it uses a form of reference
counting to track the roots for each garbage collected object. Second, it has a
naive implementation of the mark-sweep algorithm and does not use parallel
collector threads.


\section{Related work}
\label{sec:related_work}

\subsection{How other languages deal with finalisers}

The D programming language uses a conservative mark-sweep GC for heap
allocations by default with support for opt-in explicit deallocation. Like \ourgc, D does
not specify an order for finalisers. Users can specify their own allocators for
RAII-based heap allocation using standard \lstinline{malloc}/\lstinline{free}
calls.

Oilpan, the conservative mark-sweep garbage collector in Chrome's rendering
engine, Blink, uses two levels of finalisation: full finalisation, which happen
off-thread after a GC cycle has completed but do not allow object fields to be
dereferenced; and pre-finalisers, which happen during the sweep phase, but allow
all of an objects fields to be dereferenced. Pre-finalisers are generally
avoided because of their performance overhead since they cannot be scheduled to
run on other threads and increase the stop-the-world time.

\jake{TODO: Nim}


\subsection{GC in Rust}

Throughout Rust's history, there have been several attempts to introduce some
form of tracing garbage collection~\citep{felix15specifying, felix16roots,
manish16gc}. In fact, early versions of Rust explored
using a form of this as a first class feature of the language through the use of
\emph{managed pointers} (with the syntax \lstinline{@T}). This was removed
fairly early in Rust's development before the first stable release, and was only
implemented as reference counting. Since then, there have been
several attempts at a more advanced form of GC than the reference counting
library \lstinline{Rc} to the language.\laurie{cite the last commit which removes it} \jake{This one? \lstinline{https://github.com/rust-lang/rust/commit/ade807c6dcf6dc4454732c5e914ca06ebb429773}}

\laurie{what does \rustbacon do with finalisers?} \jake{For non-cyclic structures, this will use rust drop the same way that \lstinline{Rc} does. During the \lstinline{collect_cycles} method, after the dead cycles have been identified, there is a phase which calls all their drop methods. Once all drop methods are called, a final phase deallocates the objects.}
\rustbacon~\citep{rustbacon} is a Rust library implementation of the Bacon-Rajan
cycle collecting reference counting implementation~\citep{bacon01concurrent}.
\rustbacon is single-threaded, and provides thread-local reference counted boxes
using a \lstinline{Cc<T>} smart pointer. Though not intended as a general
purpose GC, it is designed to make it easier to manage cyclic data structures in
Rust. When a reference in \rustbacon is decremented to a non-zero value, it is
added to a worklist so that it can later be checked for potential cycles when
the user manually invokes the \lstinline{collect_cycles} function.

The major limitation of \rustbacon is that it is limited to a single thread
(though a concurrent cycle detection algorithm is
possible~\citep{bacon01concurrent}). However, unlike \ourgc, \rustbacon is
purely library based: it does not require any modifications to the Rust compiler
to work. In addition, as a reference counted approach, it still maintains the
advantages and disadvantages of reference counting over \ourgc's tracing GC --
except, of course, that it can collect cycles \laurie{if it can collect cycles,
then it has none of the disadvantages of Rc, surely?}! \jake{Poor phrasing perhaps, but it has the other disadvantages of RC: incref/decref cost; machine word for strong count; read-only operations become writes (cache implications) etc}

\rustgc~\citep{rustgc} is library for Rust which provides optional
single-threaded mark-sweep GC with the \lstinline{Gc<T>} type. The API for
\rustgc is similar to \ourgc, with the notable exception that \lstinline{Gc} in
\rustgc does not implement the \lstinline{Copy} trait. This means that in order
to obtain additional pointers to garbage-collected objects, the \lstinline{Gc}
must be cloned.

\laurie{what is the practical difference between \rustbacon and \rustgc?
from the description they sound identical from a user's perspective} \jake{the main difference is that a cycle collection must be explicitly invoked by the user for \rustbacon}
\rustgc is implemented as a hybrid form of reference counting and tracing GC.
There is no mechanism for scanning the stack for roots as in traditional GC, so
roots are tracked using reference counting, with a mark-sweep then performed
from these roots. Like \ourgc, \lstinline{Gc} references in \rustgc point to an
underlying \lstinline{GcBox}. However, in \rustgc, this \lstinline{GcBox}
maintains a count of all of its roots. \autoref{lst:rustgc_roots} shows how this
count is updated as references are used. During a collection, the
\lstinline{GcBox}'s on the heap are enumerated, and those with a non-zero root
count are used as roots to begin marking. Like \rustbacon, \rustgc traces
through objects by requiring types used in \lstinline{Gc} to implement a
\lstinline{Trace} trait, which has a \lstinline{trace} method called during
marking to traverse and mark objects during a collection.

\rustgc makes implementing \lstinline{Trace} easy by providing a macro
implemention, where types can be annotated with \lstinline{#[derive(Trace)]} and
have it implemented automatically. It uses its own type, \lstinline{GcCell} in
order to support interior mutability as a \lstinline{RefCell} cannot be used
with \rustgc. The \lstinline{GcCell} provides additional support for rooting and
unrooting objects across a borrow as they are mutated inside the \lstinline{Gc}.
It provides a similar API to the user as \lstinline{RefCell}.

Unlike \ourgc, objects are finalised by implementing a \lstinline{Finalize}
trait. Though this reduces much of the complexity that \ourgc needs in order to
support calling \lstinline{T::drop} from a finaliser or destructor context,
\rustgc requires the programmer to ensure that a finaliser implementation is
present for any type that may need to call \lstinline{Drop} on any of its
component types. It is not easy to know which of these component types may need
dropping, and forgetting to do so can cause memory leaks.

The \emph{Bronze} collector is an optional GC implementation for Rust which was
designed address usability concerns with Rust's borrow
semantics~\citep{coblenz21bronze}. It was
designed alongside an empirical study which measured how long it took students
to complete a variety of Rust tasks by using standalone Rust, and Rust with
Bronze for managing memory.

Bronze bases much of its implementation on \rustgc but with two key differences.
First, it tracks roots to GC objects by using a modified version of the Rust
compiler. Bronze's rustgc implementation inserts calls to LLVM's
\lstinline{gc.root} intrinsic at function entries in order to generate
stackmaps. When a GC call is requested, Bronze iterates over the stackmaps
generated its current call stack in order to locate the roots for garbage
collection. However, Bronze does not implement this for transitive references
from arbitrary objects. In other words, if a \lstinline{Gc<T>} exists as a field
inside another object instead of directly on the stack, it is not tracked
as a root for garbage collection.

The second major difference between \rustgc and Bronze is that Bronze's
\lstinline{Gc<T>} type allows the programmer to dereference its underlying type
\lstinline{T} mutably more than once. \citet{coblenz21bronze} describes this
as beneficial, because it makes it easier to use than other Rust shared
ownership. However, this is fundamentally unsound, and allows programs which
violate memory safety to be written in safe Rust using Bronze.
\autoref{lst:bronze_unsound} shows an example of how this can violate memory
safety by causing a write from deallocated memory.


\begin{figure}[!t]
\begin{lstrustsmall}
fn main() {
    let mut gr1 = GcRef::new(vec![1u16,2,3]);
    let mut gr2 = gr1.clone();

    let ref1 = gr1.as_mut();
    let ref2 = gr2.as_mut();

    // ref1 and ref2 now reference the same object:
    ref1.push(4);
    ref2.push(5);
    ref1.push(6);

    let ref1elem0 = ref1.get_mut(0).unwrap();
    // Force reallocation of the underlying vec
    ref2.resize(1024, 0);
    // Now this writes to deallocated memory
    *ref1elem0 = 42;
}
\end{lstrustsmall}
    \caption{An example of unsoundness in Bronze based on its ability to allow
    aliased mutable references. Here, we obtain two mutable references to the
    same underlying vector (lines 5-6), before using the second reference to
    resize the vector, which forces its backing store to be reallocated in
    memory (line 15). Later, when we try to access an element through the first
    reference, it no longer points to valid memory (line 15).
    } \label{lst:bronze_unsound}
\end{figure}

\shifgrethor~\citep{shifgrethor} is an experimental GC API for Rust which investigated a way for
potential GC implemenations to precisely identify and trace roots to GC'd
objects. \shifgrethor is therefore not a full GC library, but instead an
experimental design for how a GC could interface with the language.

The basic idea is that in order to create a \lstinline{Gc} object, it must be
created by, and exist alongside a corresponding \lstinline{Root<'root>} type on
the stack. The \lstinline{Root<'root>} can then dish out references to the
underlying \lstinline{Gc} which are tied to \lstinline{Root<'root>}'s lifetime.

\laurie{can \gcarena handle cycles? what about finalisers?}
\gcarena~\citep{gcarena} is another experimental approach at sound GC design in Rust. It was
originally developed as part of the \emph{luster} VM~\citep{luster}: an experimental Lua VM
written in Rust. Unlike \ourgc and the other approaches to GC in Rust seen so
far, \gcarena does not retrofit Rust with a GC. Instead, it provides limited
garbage collection in isolated garbage collected \emph{arenas}. Arenas carefully
guard mutator access to their objects through closures, which, when executing,
prevent the collector from running. This solves the difficult problem of finding
roots which reside on the stack: when an arena is \emph{closed} to the mutator,
no stack roots exist, so a collection can be safely scheduled. A single arena
may contain several garbage collected objects, but they cannot be
transferred between other arenas.

Because \gcarena is so different in nature to the other GCs described in this
chapter, it is difficult to compare it ergonomically to other approaches.

\section{New performance numbers}
\subsection{Performance}
\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{graphs/som_rs_perf.pdf}
    \caption{Results from the \somrs micro-benchmark experiment, where the SOM
    benchmark suite is run with both configurations: \somrsrc, and \somrsgc.
    Each benchmark is run for 30 process executions, where the error bars
    represent 99\% confidence intervals.}
\label{graph:som_rs_finalisers}
\end{figure}

\subsection{Finaliser elision}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{graphs/som_rs_finalisers.pdf}
    \caption{Results of a performance of \somrs using two configurations: naive
    finalisation (where no optimisation is performed); and \ourgc's finaliser
    elision optimisation (where finalisers which are used only to deallocate
    memory are removed). Each benchmark is run for 30 process executions, where
    the error bars represent 99\% confidence intervals.}
\label{graph:som_rs_finalisers}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{graphs/yksom_finalisers.pdf}
    \caption{Results of a performance of \yksom using two configurations: naive
    finalisation (where no optimisation is performed); and \ourgc's finaliser
    elision optimisation (where finalisers which are used only to deallocate
    memory are removed). Each benchmark is run for 30 process executions, where
    the error bars represent 99\% confidence intervals.}
\label{graph:yksom_finalisers}
\end{figure}

\subsection{Early finaliser prevention}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\textwidth]{graphs/som_rs_barriers.pdf}
     \caption{Results showing the performance of early finaliser prevention on
     \somrs, which uses three configurations: \textit{None} where there are
     no compiler barriers (which is unsound); \textit{All}, where every
     single \lstinline{Gc} reference has a corresponding barrier; and
     \textit{None}, where barriers can be optimised away where \ourgc is
     certain they are unnecessary. Each configuration is compared using a
     subset of benchmarks on the Rebench benchmarking suite for 30 process
     executions, where I report 99\% confidence intervals.}
\label{graph:yksom_finalisers}
\end{figure}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\end{document}
